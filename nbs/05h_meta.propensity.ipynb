{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp meta.propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metalearner Propensity\n",
    "\n",
    "> Metalearner Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# REFERENCE: https://github.com/uber/causalml\n",
    "\n",
    "# Copyright 2019 Uber Technology, Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import logging\n",
    "import numpy as np\n",
    "from pygam import LogisticGAM, s\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "logger = logging.getLogger('causalnlp')\n",
    "\n",
    "\n",
    "class PropensityModel(metaclass=ABCMeta):\n",
    "    def __init__(self, clip_bounds=(1e-3, 1 - 1e-3), **model_kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip_bounds (tuple): lower and upper bounds for clipping propensity scores. Bounds should be implemented\n",
    "                    such that: 0 < lower < upper < 1, to avoid division by zero in BaseRLearner.fit_predict() step.\n",
    "            model_kwargs: Keyword arguments to be passed to the underlying classification model.\n",
    "        \"\"\"\n",
    "        self.clip_bounds = clip_bounds\n",
    "        self.model_kwargs = model_kwargs\n",
    "        self.model = self._model\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _model(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.model.__repr__()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a propensity model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): a feature matrix\n",
    "            y (numpy.ndarray): a binary target vector\n",
    "        \"\"\"\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict propensity scores.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): a feature matrix\n",
    "\n",
    "        Returns:\n",
    "            (numpy.ndarray): Propensity scores between 0 and 1.\n",
    "        \"\"\"\n",
    "        return np.clip(\n",
    "            self.model.predict_proba(X)[:, 1], *self.clip_bounds\n",
    "        )\n",
    "\n",
    "    def fit_predict(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a propensity model and predict propensity scores.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): a feature matrix\n",
    "            y (numpy.ndarray): a binary target vector\n",
    "\n",
    "        Returns:\n",
    "            (numpy.ndarray): Propensity scores between 0 and 1.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        propensity_scores = self.predict(X)\n",
    "        logger.info('AUC score: {:.6f}'.format(auc(y, propensity_scores)))\n",
    "        return propensity_scores\n",
    "\n",
    "\n",
    "class LogisticRegressionPropensityModel(PropensityModel):\n",
    "    \"\"\"\n",
    "    Propensity regression model based on the LogisticRegression algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _model(self):\n",
    "        kwargs = {\n",
    "            'penalty': 'elasticnet',\n",
    "            'solver': 'saga',\n",
    "            'Cs': np.logspace(1e-3, 1 - 1e-3, 4),\n",
    "            'l1_ratios': np.linspace(1e-3, 1 - 1e-3, 4),\n",
    "            'cv': StratifiedKFold(\n",
    "                n_splits=self.model_kwargs.pop('n_fold') if 'n_fold' in self.model_kwargs else 4,\n",
    "                shuffle=True,\n",
    "                random_state=self.model_kwargs.get('random_state', 42)\n",
    "            ),\n",
    "            'random_state': 42,\n",
    "        }\n",
    "        kwargs.update(self.model_kwargs)\n",
    "\n",
    "        return LogisticRegressionCV(**kwargs)\n",
    "    \n",
    "class SimplePropensityModel(PropensityModel):\n",
    "    \"\"\"\n",
    "    Propensity regression model based on the LogisticRegression algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _model(self):\n",
    "        kwargs = {'max_iter': 100}\n",
    "        kwargs.update(self.model_kwargs)\n",
    "        return LogisticRegression(**kwargs)\n",
    "\n",
    "\n",
    "class ElasticNetPropensityModel(LogisticRegressionPropensityModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "class GradientBoostedPropensityModel(PropensityModel):\n",
    "    \"\"\"\n",
    "    Gradient boosted propensity score model with optional early stopping.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Please see the xgboost documentation for more information on gradient boosting tuning parameters:\n",
    "    https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        early_stop=False,\n",
    "        clip_bounds=(1e-3, 1 - 1e-3),\n",
    "        **model_kwargs\n",
    "    ):\n",
    "        super(GradientBoostedPropensityModel, self).__init__(clip_bounds, **model_kwargs)\n",
    "        self.early_stop = early_stop\n",
    "\n",
    "    @property\n",
    "    def _model(self):\n",
    "        kwargs = {\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'objective': 'binary:logistic',\n",
    "            'nthread': -1,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "        }\n",
    "        kwargs.update(self.model_kwargs)\n",
    "\n",
    "        return xgb.XGBClassifier(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, early_stopping_rounds=10, stop_val_size=0.2):\n",
    "        \"\"\"\n",
    "        Fit a propensity model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): a feature matrix\n",
    "            y (numpy.ndarray): a binary target vector\n",
    "        \"\"\"\n",
    "\n",
    "        if self.early_stop:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=stop_val_size\n",
    "            )\n",
    "\n",
    "            self.model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=early_stopping_rounds\n",
    "            )\n",
    "        else:\n",
    "            super(GradientBoostedPropensityModel, self).fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict propensity scores.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): a feature matrix\n",
    "\n",
    "        Returns:\n",
    "            (numpy.ndarray): Propensity scores between 0 and 1.\n",
    "        \"\"\"\n",
    "        if self.early_stop:\n",
    "            return np.clip(\n",
    "                self.model.predict_proba(\n",
    "                    X,\n",
    "                    ntree_limit=self.model.best_ntree_limit\n",
    "                )[:, 1],\n",
    "                *self.clip_bounds\n",
    "            )\n",
    "        else:\n",
    "            return super(GradientBoostedPropensityModel, self).predict(X)\n",
    "\n",
    "\n",
    "def calibrate(ps, treatment):\n",
    "    \"\"\"Calibrate propensity scores with logistic GAM.\n",
    "\n",
    "    Ref: https://pygam.readthedocs.io/en/latest/api/logisticgam.html\n",
    "\n",
    "    Args:\n",
    "        ps (numpy.array): a propensity score vector\n",
    "        treatment (numpy.array): a binary treatment vector (0: control, 1: treated)\n",
    "\n",
    "    Returns:\n",
    "        (numpy.array): a calibrated propensity score vector\n",
    "    \"\"\"\n",
    "\n",
    "    gam = LogisticGAM(s(0)).fit(ps, treatment)\n",
    "\n",
    "    return gam.predict_proba(ps)\n",
    "\n",
    "\n",
    "def compute_propensity_score(X, treatment, p_model=None, X_pred=None, treatment_pred=None, calibrate_p=True):\n",
    "    \"\"\"Generate propensity score if user didn't provide\n",
    "\n",
    "    Args:\n",
    "        X (np.matrix): features for training\n",
    "        treatment (np.array or pd.Series): a treatment vector for training\n",
    "        p_model (propensity model object, optional):\n",
    "            ElasticNetPropensityModel (default) / GradientBoostedPropensityModel\n",
    "        X_pred (np.matrix, optional): features for prediction\n",
    "        treatment_pred (np.array or pd.Series, optional): a treatment vector for prediciton\n",
    "        calibrate_p (bool, optional): whether calibrate the propensity score\n",
    "\n",
    "    Returns:\n",
    "        (tuple)\n",
    "            - p (numpy.ndarray): propensity score\n",
    "            - p_model (PropensityModel): a trained PropensityModel object\n",
    "    \"\"\"\n",
    "    if treatment_pred is None:\n",
    "        treatment_pred = treatment.copy()\n",
    "    if p_model is None:\n",
    "        #p_model = ElasticNetPropensityModel(max_iter=10000)\n",
    "        p_model = SimplePropensityModel(max_iter=10000)\n",
    "    print('computing propensity scores...')\n",
    "    p_model.fit(X, treatment)\n",
    "    print('done.')\n",
    "\n",
    "    if X_pred is None:\n",
    "        p = p_model.predict(X)\n",
    "    else:\n",
    "        p = p_model.predict(X_pred)\n",
    "\n",
    "    if calibrate_p:\n",
    "        logger.info('Calibrating propensity scores.')\n",
    "        p = calibrate(p, treatment_pred)\n",
    "\n",
    "    # force the p values within the range\n",
    "    eps = np.finfo(float).eps\n",
    "    p = np.where(p < 0 + eps, 0 + eps*1.001, p)\n",
    "    p = np.where(p > 1 - eps, 1 - eps*1.001, p)\n",
    "\n",
    "    return p, p_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted 02_analyzers.ipynb.\n",
      "Converted 03_key_driver_analysis.ipynb.\n",
      "Converted 04_preprocessing.ipynb.\n",
      "Converted 05a_meta.base.ipynb.\n",
      "Converted 05b_meta.explainer.ipynb.\n",
      "Converted 05c_meta.utils.ipynb.\n",
      "Converted 05d_meta.propensity.ipynb.\n",
      "Converted 99_examples.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
