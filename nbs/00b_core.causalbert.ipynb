{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: CausalBert API\n",
    "output-file: core.causalbert.html\n",
    "title: CausalBert\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core.causalbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# An extensible implementation of the Causal Bert model from \n",
    "# \"Adapting Text Embeddings for Causal Inference\" \n",
    "#    (https://arxiv.org/abs/1905.12741)\n",
    "#This implementation has been adapted from this GitHub repository:\n",
    "#    https://github.com/rpryzant/causal-bert-pytorch\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    raise ImportError('CausalBert requires PyTorch. Please install it.')\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from transformers.optimization import  AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.models.distilbert import DistilBertModel, DistilBertPreTrainedModel, DistilBertTokenizer\n",
    "\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from scipy.special import logit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "CUDA = (torch.cuda.device_count() > 0)\n",
    "MASK_IDX = 103\n",
    "\n",
    "\n",
    "def platt_scale(outcome, probs):\n",
    "    logits = logit(probs)\n",
    "    logits = logits.reshape(-1, 1)\n",
    "    log_reg = LogisticRegression(penalty='none', warm_start=True, solver='lbfgs')\n",
    "    log_reg.fit(logits, outcome)\n",
    "    return log_reg.predict_proba(logits)\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def make_bow_vector(ids, vocab_size, use_counts=False):\n",
    "    \"\"\" Make a sparse BOW vector from a tensor of dense ids.\n",
    "    Args:\n",
    "        ids: torch.LongTensor [batch, features]. Dense tensor of ids.\n",
    "        vocab_size: vocab size for this tensor.\n",
    "        use_counts: if true, the outgoing BOW vector will contain\n",
    "            feature counts. If false, will contain binary indicators.\n",
    "    Returns:\n",
    "        The sparse bag-of-words representation of ids.\n",
    "    \"\"\"\n",
    "    vec = torch.zeros(ids.shape[0], vocab_size)\n",
    "    ones = torch.ones_like(ids, dtype=torch.float)\n",
    "    if CUDA:\n",
    "        vec = vec.cuda()\n",
    "        ones = ones.cuda()\n",
    "        ids = ids.cuda()\n",
    "\n",
    "    vec.scatter_add_(1, ids, ones)\n",
    "    vec[:, 1] = 0.0  # zero out pad\n",
    "    if not use_counts:\n",
    "        vec = (vec != 0).float()\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "class CausalBert(DistilBertPreTrainedModel):\n",
    "    \"\"\"CausalBert is essentially an S-Learner that uses a DistilBert sequence classification model as the base learner.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.vocab_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n",
    "\n",
    "        self.Q_cls = nn.ModuleDict()\n",
    "\n",
    "        for T in range(2):\n",
    "            # ModuleDict keys have to be strings..\n",
    "            self.Q_cls['%d' % T] = nn.Sequential(\n",
    "                nn.Linear(config.hidden_size + self.num_labels, 200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, self.num_labels))\n",
    "\n",
    "        self.g_cls = nn.Linear(config.hidden_size + self.num_labels, \n",
    "            self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, W_ids, W_len, W_mask, C, T, Y=None, use_mlm=True):\n",
    "        if use_mlm:\n",
    "            W_len = W_len.unsqueeze(1) - 2 # -2 because of the +1 below\n",
    "            mask_class = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "            mask = (mask_class(W_len.shape).uniform_() * W_len.float()).long() + 1 # + 1 to avoid CLS\n",
    "            target_words = torch.gather(W_ids, 1, mask)\n",
    "            mlm_labels = torch.ones(W_ids.shape).long() * -100\n",
    "            if CUDA:\n",
    "                mlm_labels = mlm_labels.cuda()\n",
    "            mlm_labels.scatter_(1, mask, target_words)\n",
    "            W_ids.scatter_(1, mask, MASK_IDX)\n",
    "\n",
    "        outputs = self.distilbert(W_ids, attention_mask=W_mask)\n",
    "        seq_output = outputs[0]\n",
    "        pooled_output = seq_output[:, 0]\n",
    "        # seq_output, pooled_output = outputs[:2]\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        if use_mlm:\n",
    "            prediction_logits = self.vocab_transform(seq_output)  # (bs, seq_length, dim)\n",
    "            prediction_logits = gelu(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "            mlm_loss = CrossEntropyLoss()(\n",
    "                prediction_logits.view(-1, self.vocab_size), mlm_labels.view(-1))\n",
    "        else:\n",
    "            mlm_loss = 0.0\n",
    "\n",
    "        C_bow = make_bow_vector(C.unsqueeze(1), self.num_labels)\n",
    "        inputs = torch.cat((pooled_output, C_bow), 1)\n",
    "\n",
    "        # g logits\n",
    "        g = self.g_cls(inputs)\n",
    "        if Y is not None:  # TODO train/test mode, this is a lil hacky\n",
    "            g_loss = CrossEntropyLoss()(g.view(-1, self.num_labels), T.view(-1))\n",
    "        else:\n",
    "            g_loss = 0.0\n",
    "\n",
    "        # conditional expected outcome logits: \n",
    "        # run each example through its corresponding T matrix\n",
    "        # TODO this would be cleaner with sigmoid and BCELoss, but less general \n",
    "        #   (and I couldn't get it to work as well)\n",
    "        Q_logits_T0 = self.Q_cls['0'](inputs)\n",
    "        Q_logits_T1 = self.Q_cls['1'](inputs)\n",
    "\n",
    "        if Y is not None:\n",
    "            T0_indices = (T == 0).nonzero().squeeze()\n",
    "            Y_T1_labels = Y.clone().scatter(0, T0_indices, -100)\n",
    "\n",
    "            T1_indices = (T == 1).nonzero().squeeze()\n",
    "            Y_T0_labels = Y.clone().scatter(0, T1_indices, -100)\n",
    "\n",
    "            Q_loss_T1 = CrossEntropyLoss()(\n",
    "                Q_logits_T1.view(-1, self.num_labels), Y_T1_labels)\n",
    "            Q_loss_T0 = CrossEntropyLoss()(\n",
    "                Q_logits_T0.view(-1, self.num_labels), Y_T0_labels)\n",
    "\n",
    "            Q_loss = Q_loss_T0 + Q_loss_T1\n",
    "        else:\n",
    "            Q_loss = 0.0\n",
    "\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        Q0 = sm(Q_logits_T0)[:, 1]\n",
    "        Q1 = sm(Q_logits_T1)[:, 1]\n",
    "        g = sm(g)[:, 1]\n",
    "\n",
    "        return g, Q0, Q1, g_loss, Q_loss, mlm_loss\n",
    "\n",
    "\n",
    "\n",
    "class CausalBertModel:\n",
    "    \"\"\"CausalBertModel is a wrapper for CausalBert\"\"\"\n",
    "\n",
    "    def __init__(self, g_weight=0.0, Q_weight=0.1, mlm_weight=1.0,\n",
    "        batch_size=32, max_length=128, model_name=\"distilbert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Trains a model to predict a binary outcome taking into account\n",
    "        the text, the treatment, and a single binary confounder.\n",
    "        The resultant model can be used to estimate treatment effects\n",
    "        for observations.\n",
    "        \"\"\"\n",
    "        if 'distilbert' not in model_name: \n",
    "            raise ValueError('CausalBert currently only supports DistilBERT models')\n",
    "        self.model = CausalBert.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False)\n",
    "        if CUDA:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        self.loss_weights = {\n",
    "            'g': g_weight,\n",
    "            'Q': Q_weight,\n",
    "            'mlm': mlm_weight\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def train(self, texts, confounds, treatments, outcomes,\n",
    "            learning_rate=2e-5, epochs=3):\n",
    "        \"\"\"\n",
    "        Trains a CausalBert model\n",
    "        \"\"\"\n",
    "        dataloader = self.build_dataloader(\n",
    "            texts, confounds, treatments, outcomes)\n",
    "\n",
    "        self.model.train()\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "        total_steps = len(dataloader) * epochs\n",
    "        warmup_steps = total_steps * 0.1\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            self.model.train()\n",
    "            for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                    if CUDA: \n",
    "                        batch = (x.cuda() for x in batch)\n",
    "                    W_ids, W_len, W_mask, C, T, Y = batch\n",
    "                    # while True:\n",
    "                    self.model.zero_grad()\n",
    "                    g, Q0, Q1, g_loss, Q_loss, mlm_loss = self.model(W_ids, W_len, W_mask, C, T, Y)\n",
    "                    loss = self.loss_weights['g'] * g_loss + \\\n",
    "                            self.loss_weights['Q'] * Q_loss + \\\n",
    "                            self.loss_weights['mlm'] * mlm_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    losses.append(loss.detach().cpu().item())\n",
    "                # print(np.mean(losses))\n",
    "                    # if step > 5: continue\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def inference(self, texts, confounds, outcome=None):\n",
    "        \"\"\"\n",
    "        Perform inference using the trained model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        dataloader = self.build_dataloader(texts, confounds, outcomes=outcome,\n",
    "            sampler='sequential')\n",
    "        Q0s = []\n",
    "        Q1s = []\n",
    "        Ys = []\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            if CUDA: \n",
    "                batch = (x.cuda() for x in batch)\n",
    "            W_ids, W_len, W_mask, C, T, Y = batch\n",
    "            g, Q0, Q1, _, _, _ = self.model(W_ids, W_len, W_mask, C, T, use_mlm=False)\n",
    "            Q0s += Q0.detach().cpu().numpy().tolist()\n",
    "            Q1s += Q1.detach().cpu().numpy().tolist()\n",
    "            Ys += Y.detach().cpu().numpy().tolist()\n",
    "            # if i > 5: break\n",
    "        probs = np.array(list(zip(Q0s, Q1s)))\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "\n",
    "        return probs, preds, Ys\n",
    "\n",
    "    def estimate_ate(self, C, W, Y=None, platt_scaling=False):\n",
    "        \"\"\"\n",
    "        Computes average treatment effect using the trained estimator\n",
    "        \"\"\"\n",
    "        Q_probs, _, Ys = self.inference(W, C, outcome=Y)\n",
    "        if platt_scaling and Y is not None:\n",
    "            Q0 = platt_scale(Ys, Q_probs[:, 0])[:, 0]\n",
    "            Q1 = platt_scale(Ys, Q_probs[:, 1])[:, 1]\n",
    "        else:\n",
    "            Q0 = Q_probs[:, 0]\n",
    "            Q1 = Q_probs[:, 1]\n",
    "\n",
    "        return np.mean(Q1 - Q0)\n",
    "\n",
    "    def build_dataloader(self, texts, confounds, treatments=None, outcomes=None,\n",
    "        tokenizer=None, sampler='random'):\n",
    "        def collate_CandT(data):\n",
    "            # sort by (C, T), so you can get boundaries later\n",
    "            # (do this here on cpu for speed)\n",
    "            data.sort(key=lambda x: (x[1], x[2]))\n",
    "            return data\n",
    "        # fill with dummy values\n",
    "        if treatments is None:\n",
    "            treatments = [-1 for _ in range(len(confounds))]\n",
    "        if outcomes is None:\n",
    "            outcomes = [-1 for _ in range(len(treatments))]\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "                'distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        out = defaultdict(list)\n",
    "        for i, (W, C, T, Y) in enumerate(zip(texts, confounds, treatments, outcomes)):\n",
    "            # out['W_raw'].append(W)\n",
    "            encoded_sent = tokenizer.encode_plus(W, add_special_tokens=True,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length')\n",
    "\n",
    "            out['W_ids'].append(encoded_sent['input_ids'])\n",
    "            out['W_mask'].append(encoded_sent['attention_mask'])\n",
    "            out['W_len'].append(sum(encoded_sent['attention_mask']))\n",
    "            out['Y'].append(Y)\n",
    "            out['T'].append(T)\n",
    "            out['C'].append(C)\n",
    "\n",
    "        data = (torch.tensor(out[x]) for x in ['W_ids', 'W_len', 'W_mask', 'C', 'T', 'Y'])\n",
    "        data = TensorDataset(*data)\n",
    "        sampler = RandomSampler(data) if sampler == 'random' else SequentialSampler(data)\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=self.batch_size)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CausalBertModel.train\" class=\"doc_header\"><code>CausalBertModel.train</code><a href=\"__main__.py#L200\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CausalBertModel.train</code>(**`texts`**, **`confounds`**, **`treatments`**, **`outcomes`**, **`learning_rate`**=*`2e-05`*, **`epochs`**=*`3`*)\n",
       "\n",
       "Trains a CausalBert model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CausalBertModel.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CausalBertModel.estimate_ate\" class=\"doc_header\"><code>CausalBertModel.estimate_ate</code><a href=\"__main__.py#L261\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CausalBertModel.estimate_ate</code>(**`C`**, **`W`**, **`Y`**=*`None`*, **`platt_scaling`**=*`False`*)\n",
       "\n",
       "Computes average treatment effect using the trained estimator"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CausalBertModel.estimate_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CausalBertModel.inference\" class=\"doc_header\"><code>CausalBertModel.inference</code><a href=\"__main__.py#L237\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CausalBertModel.inference</code>(**`texts`**, **`confounds`**, **`outcome`**=*`None`*)\n",
       "\n",
       "Perform inference using the trained model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CausalBertModel.inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "This implementation of `CausalBert` was adapted from [Causal Effects of Linguistic Properties](https://arxiv.org/abs/2010.12919) by Pryzant et al.  `CausalBert` is essentially a kind of [S-Learner](https://arxiv.org/abs/1706.03461) that uses a DistilBert sequence classification model as the base learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "import pandas as pd\n",
    "from causalnlp.core.causalbert import CausalBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CausalBert were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['Q_cls.1.0.bias', 'Q_cls.0.0.bias', 'g_cls.weight', 'Q_cls.1.0.weight', 'g_cls.bias', 'Q_cls.1.2.bias', 'Q_cls.0.2.weight', 'Q_cls.0.0.weight', 'Q_cls.0.2.bias', 'Q_cls.1.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 666/666 [02:12<00:00,  5.01it/s]\n",
      "100%|██████████| 666/666 [00:27<00:00, 24.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17478953341997637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "df = pd.read_csv('sample_data/music_seed50.tsv', sep='\\t', on_bad_lines='skip')\n",
    "cb = CausalBertModel(batch_size=32, max_length=128)\n",
    "cb.train(df['text'], df['C_true'], df['T_ac'], df['Y_sim'], epochs=1, learning_rate=2e-5)\n",
    "print(cb.estimate_ate(df['C_true'], df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Reduce the `batch_size` if you receive an Out-Of-Memory error when running the code above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00a_core.causalinference.ipynb.\n",
      "Converted 00b_core.causalbert.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted 02_analyzers.ipynb.\n",
      "Converted 03_key_driver_analysis.ipynb.\n",
      "Converted 04_preprocessing.ipynb.\n",
      "Converted 05a_meta.base.ipynb.\n",
      "Converted 05b_meta.tlearner.ipynb.\n",
      "Converted 05c_meta.slearner.ipynb.\n",
      "Converted 05d_meta.xlearner.ipynb.\n",
      "Converted 05e_meta.rlearner.ipynb.\n",
      "Converted 05f_meta.utils.ipynb.\n",
      "Converted 05g_meta.explainer.ipynb.\n",
      "Converted 05h_meta.propensity.ipynb.\n",
      "Converted 05i_meta.sensitivity.ipynb.\n",
      "Converted 99_examples.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
