{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Metalearner Sensitivity\n",
    "output-file: meta.sensitivity.html\n",
    "title: Metalearner Sensitivity\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp meta.sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# REFERENCE: https://github.com/uber/causalml\n",
    "\n",
    "# Copyright 2019 Uber Technology, Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "logger = logging.getLogger('sensitivity')\n",
    "\n",
    "\n",
    "def one_sided(alpha, p, treatment):\n",
    "    \"\"\"One sided confounding function.\n",
    "    Reference:  Blackwell, Matthew. \"A selection bias approach to sensitivity analysis\n",
    "    for causal effects.\" Political Analysis 22.2 (2014): 169-182.\n",
    "    https://www.mattblackwell.org/files/papers/causalsens.pdf\n",
    "\n",
    "    Args:\n",
    "        alpha (np.array): a confounding values vector\n",
    "        p (np.array): a propensity score vector between 0 and 1\n",
    "        treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "    \"\"\"\n",
    "    assert p.shape[0] == treatment.shape[0]\n",
    "    adj = alpha * (1 - p) * treatment - alpha * p * (1 - treatment)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def alignment(alpha, p, treatment):\n",
    "    \"\"\"Alignment confounding function.\n",
    "    Reference:  Blackwell, Matthew. \"A selection bias approach to sensitivity analysis\n",
    "    for causal effects.\" Political Analysis 22.2 (2014): 169-182.\n",
    "    https://www.mattblackwell.org/files/papers/causalsens.pdf\n",
    "\n",
    "    Args:\n",
    "        alpha (np.array): a confounding values vector\n",
    "        p (np.array): a propensity score vector between 0 and 1\n",
    "        treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "    \"\"\"\n",
    "\n",
    "    assert p.shape[0] == treatment.shape[0]\n",
    "    adj = alpha * (1 - p) * treatment + alpha * p * (1 - treatment)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def one_sided_att(alpha, p, treatment):\n",
    "    \"\"\"One sided confounding function for the average effect of the treatment among the treated units (ATT)\n",
    "\n",
    "    Reference:  Blackwell, Matthew. \"A selection bias approach to sensitivity analysis\n",
    "    for causal effects.\" Political Analysis 22.2 (2014): 169-182.\n",
    "    https://www.mattblackwell.org/files/papers/causalsens.pdf\n",
    "\n",
    "    Args:\n",
    "        alpha (np.array): a confounding values vector\n",
    "        p (np.array): a propensity score vector between 0 and 1\n",
    "        treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "    \"\"\"\n",
    "    assert p.shape[0] == treatment.shape[0]\n",
    "    adj = alpha * (1 - treatment)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def alignment_att(alpha, p, treatment):\n",
    "    \"\"\"Alignment confounding function for the average effect of the treatment among the treated units (ATT)\n",
    "\n",
    "    Reference:  Blackwell, Matthew. \"A selection bias approach to sensitivity analysis\n",
    "    for causal effects.\" Political Analysis 22.2 (2014): 169-182.\n",
    "    https://www.mattblackwell.org/files/papers/causalsens.pdf\n",
    "\n",
    "    Args:\n",
    "        alpha (np.array): a confounding values vector\n",
    "        p (np.array): a propensity score vector between 0 and 1\n",
    "        treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "    \"\"\"\n",
    "    assert p.shape[0] == treatment.shape[0]\n",
    "    adj = alpha * (1 - treatment)\n",
    "    return adj\n",
    "\n",
    "\n",
    "class Sensitivity(object):\n",
    "    \"\"\" A Sensitivity Check class to support Placebo Treatment, Irrelevant Additional Confounder\n",
    "    and Subset validation refutation methods to verify causal inference.\n",
    "\n",
    "    Reference: https://github.com/microsoft/dowhy/blob/master/dowhy/causal_refuters/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, inference_features, p_col, treatment_col, outcome_col,\n",
    "                learner, *args, **kwargs):\n",
    "        \"\"\"Initialize.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): input data frame\n",
    "            inferenece_features (list of str): a list of columns that used in learner for inference\n",
    "            p_col (str): column name of propensity score\n",
    "            treatment_col (str): column name of whether in treatment of control\n",
    "            outcome_col (str): column name of outcome\n",
    "            learner (model): a model to estimate outcomes and treatment effects\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = df\n",
    "        self.inference_features = inference_features\n",
    "        self.p_col = p_col\n",
    "        self.treatment_col = treatment_col\n",
    "        self.outcome_col = outcome_col\n",
    "        self.learner = learner\n",
    "\n",
    "    def get_prediction(self, X, p, treatment, y):\n",
    "        \"\"\"Return the treatment effects prediction.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix): a feature matrix\n",
    "            p (np.array): a propensity score vector between 0 and 1\n",
    "            treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "            y (np.array): an outcome vector\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects\n",
    "        \"\"\"\n",
    "\n",
    "        learner = self.learner\n",
    "        try:\n",
    "            preds = learner.fit_predict(X=X, p=p, treatment=treatment, y=y).flatten()\n",
    "        except TypeError:\n",
    "            preds = learner.fit_predict(X=X, treatment=treatment, y=y).flatten()\n",
    "        return preds\n",
    "\n",
    "    def get_ate_ci(self, X, p, treatment, y):\n",
    "        \"\"\"Return the confidence intervals for treatment effects prediction.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix): a feature matrix\n",
    "            p (np.array): a propensity score vector between 0 and 1\n",
    "            treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "            y (np.array): an outcome vector\n",
    "        Returns:\n",
    "            (numpy.ndarray): Mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "\n",
    "        learner = self.learner\n",
    "        from causalnlp.meta.tlearner import BaseTLearner\n",
    "        if isinstance(learner, BaseTLearner):\n",
    "            ate, ate_lower, ate_upper = learner.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "        else:\n",
    "            try:\n",
    "                ate, ate_lower, ate_upper = learner.estimate_ate(X=X, p=p, treatment=treatment, y=y)\n",
    "            except TypeError:\n",
    "                ate, ate_lower, ate_upper = learner.estimate_ate(X=X, treatment=treatment, y=y, return_ci=True)\n",
    "        return ate[0], ate_lower[0], ate_upper[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_class_object(method_name, *args, **kwargs):\n",
    "        \"\"\"Return class object based on input method\n",
    "        Args:\n",
    "            method_name (list of str): a list of sensitivity analysis method\n",
    "        Returns:\n",
    "            (class): Sensitivy Class\n",
    "        \"\"\"\n",
    "\n",
    "        method_list = ['Placebo Treatment', 'Random Cause', 'Subset Data', 'Random Replace', 'Selection Bias']\n",
    "        class_name = 'Sensitivity' + method_name.replace(' ', '')\n",
    "\n",
    "        try:\n",
    "            getattr(import_module('causalnlp.meta.sensitivity'), class_name)\n",
    "            return getattr(import_module('causalnlp.meta.sensitivity'), class_name)\n",
    "        except AttributeError:\n",
    "            raise AttributeError('{} is not an existing method for sensitiviy analysis.'.format(method_name) +\n",
    "                              ' Select one of {}'.format(method_list))\n",
    "\n",
    "    def sensitivity_analysis(self, methods, sample_size=None,\n",
    "                             confound='one_sided', alpha_range=None):\n",
    "        \"\"\"Return the sensitivity data by different method\n",
    "\n",
    "        Args:\n",
    "            method (list of str): a list of sensitivity analysis method\n",
    "            sample_size (float, optional): ratio for subset the original data\n",
    "            confound (string, optional): the name of confouding function\n",
    "            alpha_range (np.array, optional): a parameter to pass the confounding function\n",
    "\n",
    "        Returns:\n",
    "            X (np.matrix): a feature matrix\n",
    "            p (np.array): a propensity score vector between 0 and 1\n",
    "            treatment (np.array): a treatment vector (1 if treated, otherwise 0)\n",
    "            y (np.array): an outcome vector\n",
    "        \"\"\"\n",
    "        if alpha_range is None:\n",
    "            y = self.df[self.outcome_col]\n",
    "            iqr = y.quantile(.75) - y.quantile(.25)\n",
    "            alpha_range = np.linspace(-iqr/2, iqr/2, 11)\n",
    "            if 0 not in alpha_range:\n",
    "                alpha_range = np.append(alpha_range, 0)\n",
    "        else:\n",
    "            alpha_range = alpha_range\n",
    "\n",
    "        alpha_range.sort()\n",
    "\n",
    "        summary_df = pd.DataFrame(columns=['Method', 'ATE', 'New ATE', 'New ATE LB', 'New ATE UB'])\n",
    "        for method in methods:\n",
    "            sens = self.get_class_object(method)\n",
    "            sens = sens(self.df, self.inference_features, self.p_col, self.treatment_col, self.outcome_col,\n",
    "                        self.learner, sample_size=sample_size, confound=confound, alpha_range=alpha_range)\n",
    "\n",
    "            if method == 'Subset Data':\n",
    "                method = method + '(sample size @{})'.format(sample_size)\n",
    "\n",
    "            sens_df = sens.summary(method=method)\n",
    "            summary_df = summary_df.append(sens_df)\n",
    "\n",
    "        return summary_df\n",
    "\n",
    "    def summary(self, method):\n",
    "        \"\"\"Summary report\n",
    "        Args:\n",
    "            method_name (str): sensitivity analysis method\n",
    "\n",
    "        Returns:\n",
    "            (pd.DataFrame): a summary dataframe\n",
    "        \"\"\"\n",
    "        method_name = method\n",
    "\n",
    "        X = self.df[self.inference_features].values\n",
    "        p = self.df[self.p_col].values if self.p_col is not None else None\n",
    "        treatment = self.df[self.treatment_col].values\n",
    "        y = self.df[self.outcome_col].values\n",
    "\n",
    "        preds = self.get_prediction(X, p, treatment, y)\n",
    "        ate = preds.mean()\n",
    "        ate_new, ate_new_lower, ate_new_upper = self.sensitivity_estimate()\n",
    "\n",
    "        sensitivity_summary = pd.DataFrame([method_name, ate,\n",
    "                                            ate_new, ate_new_lower, ate_new_upper]).T\n",
    "        sensitivity_summary.columns = ['Method', 'ATE', 'New ATE', 'New ATE LB', 'New ATE UB']\n",
    "        return sensitivity_summary\n",
    "\n",
    "    def sensitivity_estimate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SensitivityPlaceboTreatment(Sensitivity):\n",
    "    \"\"\"Replaces the treatment variable with a new variable randomly generated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def sensitivity_estimate(self):\n",
    "        \"\"\"Summary report\n",
    "        Args:\n",
    "           return_ci (str): sensitivity analysis method\n",
    "\n",
    "        Returns:\n",
    "            (pd.DataFrame): a summary dataframe\n",
    "        \"\"\"\n",
    "        num_rows = self.df.shape[0]\n",
    "\n",
    "        X = self.df[self.inference_features].values\n",
    "        p = self.df[self.p_col].values if self.p_col is not None else None\n",
    "        treatment_new = np.random.randint(2, size=num_rows)\n",
    "        y = self.df[self.outcome_col].values\n",
    "\n",
    "        ate_new, ate_new_lower, ate_new_upper = self.get_ate_ci(X, p, treatment_new, y)\n",
    "        return ate_new, ate_new_lower, ate_new_upper\n",
    "\n",
    "\n",
    "class SensitivityRandomCause(Sensitivity):\n",
    "    \"\"\"Adds an irrelevant random covariate to the dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def sensitivity_estimate(self):\n",
    "        num_rows = self.df.shape[0]\n",
    "        new_data = np.random.randn(num_rows)\n",
    "\n",
    "        X = self.df[self.inference_features].values\n",
    "        p = self.df[self.p_col].values if self.p_col is not None else None\n",
    "        treatment = self.df[self.treatment_col].values\n",
    "        y = self.df[self.outcome_col].values\n",
    "        X_new = np.hstack((X, new_data.reshape((-1, 1))))\n",
    "\n",
    "        ate_new, ate_new_lower, ate_new_upper = self.get_ate_ci(X_new, p, treatment, y)\n",
    "        return ate_new, ate_new_lower, ate_new_upper\n",
    "\n",
    "\n",
    "class SensitivityRandomReplace(Sensitivity):\n",
    "    \"\"\"Replaces a random covariate with an irrelevant variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if 'replaced_feature' not in kwargs:\n",
    "            replaced_feature_index = np.random.randint(len(self.inference_features))\n",
    "            self.replaced_feature = self.inference_features[replaced_feature_index]\n",
    "        else:\n",
    "            self.replaced_feature = kwargs[\"replaced_feature\"]\n",
    "\n",
    "    def sensitivity_estimate(self):\n",
    "        \"\"\"Replaces a random covariate with an irrelevant variable.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info('Replace feature {} with an random irrelevant variable'.format(self.replaced_feature))\n",
    "        df_new = self.df.copy()\n",
    "        num_rows = self.df.shape[0]\n",
    "        df_new[self.replaced_feature] = np.random.randn(num_rows)\n",
    "\n",
    "        X_new = df_new[self.inference_features].values\n",
    "        p_new = df_new[self.p_col].values if self.p_col is not None else None\n",
    "        treatment_new = df_new[self.treatment_col].values\n",
    "        y_new = df_new[self.outcome_col].values\n",
    "\n",
    "        ate_new, ate_new_lower, ate_new_upper = self.get_ate_ci(X_new, p_new, treatment_new, y_new)\n",
    "        return ate_new, ate_new_lower, ate_new_upper\n",
    "\n",
    "\n",
    "class SensitivitySubsetData(Sensitivity):\n",
    "    \"\"\"Takes a random subset of size sample_size of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sample_size = kwargs[\"sample_size\"]\n",
    "        assert (self.sample_size is not None)\n",
    "\n",
    "    def sensitivity_estimate(self):\n",
    "        df_new = self.df.sample(frac=self.sample_size).copy()\n",
    "\n",
    "        X_new = df_new[self.inference_features].values\n",
    "        p_new = df_new[self.p_col].values if self.p_col is not None else None\n",
    "        treatment_new = df_new[self.treatment_col].values\n",
    "        y_new = df_new[self.outcome_col].values\n",
    "\n",
    "        ate_new, ate_new_lower, ate_new_upper = self.get_ate_ci(X_new, p_new, treatment_new, y_new)\n",
    "        return ate_new, ate_new_lower, ate_new_upper\n",
    "\n",
    "\n",
    "class SensitivitySelectionBias(Sensitivity):\n",
    "    \"\"\"Reference:\n",
    "\n",
    "    [1] Blackwell, Matthew. \"A selection bias approach to sensitivity analysis\n",
    "    for causal effects.\" Political Analysis 22.2 (2014): 169-182.\n",
    "    https://www.mattblackwell.org/files/papers/causalsens.pdf\n",
    "\n",
    "    [2] Confouding parameter alpha_range using the same range as in:\n",
    "    https://github.com/mattblackwell/causalsens/blob/master/R/causalsens.R\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, confound='one_sided', alpha_range=None,\n",
    "                 sensitivity_features=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \"\"\"Initialize.\n",
    "\n",
    "        Args:\n",
    "            confound (string): the name of confouding function\n",
    "            alpha_range (np.array): a parameter to pass the confounding function\n",
    "            sensitivity_features (list of str): ): a list of columns that to check each individual partial r-square\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info('Only works for linear outcome models right now. Check back soon.')\n",
    "        confounding_functions = {'one_sided': one_sided,\n",
    "                                 'alignment': alignment,\n",
    "                                 'one_sided_att': one_sided_att,\n",
    "                                 'alignment_att': alignment_att}\n",
    "\n",
    "        try:\n",
    "            confound_func = confounding_functions[confound]\n",
    "        except KeyError:\n",
    "            raise NotImplementedError(f'Confounding function, {confound} is not implemented. \\\n",
    "                                        Use one of {confounding_functions.keys()}')\n",
    "\n",
    "        self.confound = confound_func\n",
    "\n",
    "        if sensitivity_features is None:\n",
    "            self.sensitivity_features = self.inference_features\n",
    "        else:\n",
    "            self.sensitivity_features = sensitivity_features\n",
    "\n",
    "        if alpha_range is None:\n",
    "            y = self.df[self.outcome_col]\n",
    "            iqr = y.quantile(.75) - y.quantile(.25)\n",
    "            self.alpha_range = np.linspace(-iqr/2, iqr/2, 11)\n",
    "            if 0 not in self.alpha_range:\n",
    "                self.alpha_range = np.append(self.alpha_range, 0)\n",
    "        else:\n",
    "            self.alpha_range = alpha_range\n",
    "\n",
    "        self.alpha_range.sort()\n",
    "\n",
    "    def causalsens(self):\n",
    "        alpha_range = self.alpha_range\n",
    "        confound = self.confound\n",
    "        df = self.df\n",
    "        X = df[self.inference_features].values\n",
    "        p = df[self.p_col].values if self.p_col is not None else None\n",
    "        treatment = df[self.treatment_col].values\n",
    "        y = df[self.outcome_col].values\n",
    "\n",
    "        preds = self.get_prediction(X, p, treatment, y)\n",
    "\n",
    "        sens_df = pd.DataFrame()\n",
    "        for a in alpha_range:\n",
    "            sens = defaultdict(list)\n",
    "            sens['alpha'] = a\n",
    "            adj = confound(a, p, treatment)\n",
    "            preds_adj = y - adj\n",
    "            s_preds = self.get_prediction(X, p, treatment, preds_adj)\n",
    "            ate, ate_lb, ate_ub = self.get_ate_ci(X, p, treatment, preds_adj)\n",
    "\n",
    "            s_preds_residul = preds_adj - s_preds\n",
    "            sens['rsqs'] = a**2*np.var(treatment)/np.var(s_preds_residul)\n",
    "            sens['New ATE'] = ate\n",
    "            sens['New ATE LB'] = ate_lb\n",
    "            sens['New ATE UB'] = ate_ub\n",
    "            sens_df = sens_df.append(pd.DataFrame(sens, index=[0]))\n",
    "\n",
    "        rss = np.sum(np.square(y - preds))\n",
    "        partial_rsqs = []\n",
    "        for feature in self.sensitivity_features:\n",
    "            df_new = df.copy()\n",
    "            X_new = df_new[self.inference_features].drop(feature, axis=1).copy()\n",
    "            y_new_preds = self.get_prediction(X_new, p, treatment, y)\n",
    "            rss_new = np.sum(np.square(y - y_new_preds))\n",
    "            partial_rsqs.append(((rss_new - rss)/rss))\n",
    "\n",
    "        partial_rsqs_df = pd.DataFrame([self.sensitivity_features, partial_rsqs]).T\n",
    "        partial_rsqs_df.columns = ['feature', 'partial_rsqs']\n",
    "\n",
    "        return sens_df, partial_rsqs_df\n",
    "\n",
    "    def summary(self, method='Selection Bias'):\n",
    "        \"\"\"Summary report for Selection Bias Method\n",
    "        Args:\n",
    "            method_name (str): sensitivity analysis method\n",
    "        Returns:\n",
    "            (pd.DataFrame): a summary dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        method_name = method\n",
    "        sensitivity_summary = self.causalsens()[0]\n",
    "        sensitivity_summary['Method'] = [method_name + ' (alpha@' + str(round(i, 5)) + ', with r-sqaure:'\n",
    "                                         for i in sensitivity_summary.alpha]\n",
    "        sensitivity_summary['Method'] = sensitivity_summary['Method'] + sensitivity_summary['rsqs'].round(5).astype(str)\n",
    "        sensitivity_summary['ATE'] = sensitivity_summary[sensitivity_summary.alpha == 0]['New ATE']\n",
    "        return sensitivity_summary[['Method', 'ATE', 'New ATE', 'New ATE LB', 'New ATE UB']]\n",
    "\n",
    "    @staticmethod\n",
    "    def plot(sens_df, partial_rsqs_df=None, type='raw', ci=False, partial_rsqs=False):\n",
    "        \"\"\"Plot the results of a sensitivity analysis against unmeasured\n",
    "        Args:\n",
    "            sens_df (pandas.DataFrame): a data frame output from causalsens\n",
    "            partial_rsqs_d (pandas.DataFrame) : a data frame output from causalsens including partial rsqure\n",
    "            type (str, optional): the type of plot to draw, 'raw' or 'r.squared' are supported\n",
    "            ci (bool, optional): whether plot confidence intervals\n",
    "            partial_rsqs (bool, optional): whether plot partial rsquare results\n",
    "         \"\"\"\n",
    "\n",
    "        if type == 'raw' and not ci:\n",
    "            fig, ax = plt.subplots()\n",
    "            y_max = round(sens_df['New ATE UB'].max()*1.1, 4)\n",
    "            y_min = round(sens_df['New ATE LB'].min()*0.9, 4)\n",
    "            x_max = round(sens_df.alpha.max()*1.1, 4)\n",
    "            x_min = round(sens_df.alpha.min()*0.9, 4)\n",
    "            plt.ylim(y_min, y_max)\n",
    "            plt.xlim(x_min, x_max)\n",
    "            ax.plot(sens_df.alpha, sens_df['New ATE'])\n",
    "        elif type == 'raw' and ci:\n",
    "            fig, ax = plt.subplots()\n",
    "            y_max = round(sens_df['New ATE UB'].max()*1.1, 4)\n",
    "            y_min = round(sens_df['New ATE LB'].min()*0.9, 4)\n",
    "            x_max = round(sens_df.alpha.max()*1.1, 4)\n",
    "            x_min = round(sens_df.alpha.min()*0.9, 4)\n",
    "            plt.ylim(y_min, y_max)\n",
    "            plt.xlim(x_min, x_max)\n",
    "            ax.fill_between(sens_df.alpha, sens_df['New ATE LB'], sens_df['New ATE UB'], color='gray', alpha=0.5)\n",
    "            ax.plot(sens_df.alpha, sens_df['New ATE'])\n",
    "        elif type == 'r.squared' and ci:\n",
    "            fig, ax = plt.subplots()\n",
    "            y_max = round(sens_df['New ATE UB'].max()*1.1, 4)\n",
    "            y_min = round(sens_df['New ATE LB'].min()*0.9, 4)\n",
    "            plt.ylim(y_min, y_max)\n",
    "            ax.fill_between(sens_df.rsqs, sens_df['New ATE LB'], sens_df['New ATE UB'], color='gray', alpha=0.5)\n",
    "            ax.plot(sens_df.rsqs, sens_df['New ATE'])\n",
    "            if partial_rsqs:\n",
    "                plt.scatter(partial_rsqs_df.partial_rsqs,\n",
    "                        list(sens_df[sens_df.alpha == 0]['New ATE']) * partial_rsqs_df.shape[0],\n",
    "                        marker='x', color=\"red\", linewidth=10)\n",
    "        elif type == 'r.squared' and not ci:\n",
    "            fig, ax = plt.subplots()\n",
    "            y_max = round(sens_df['New ATE UB'].max()*1.1, 4)\n",
    "            y_min = round(sens_df['New ATE LB'].min()*0.9, 4)\n",
    "            plt.ylim(y_min, y_max)\n",
    "            plt.plot(sens_df.rsqs, sens_df['New ATE'])\n",
    "            if partial_rsqs:\n",
    "                plt.scatter(partial_rsqs_df.partial_rsqs,\n",
    "                        list(sens_df[sens_df.alpha == 0]['New ATE']) * partial_rsqs_df.shape[0],\n",
    "                        marker='x', color=\"red\", linewidth=10)\n",
    "\n",
    "    @staticmethod\n",
    "    def partial_rsqs_confounding(sens_df, feature_name, partial_rsqs_value, range=0.01):\n",
    "        \"\"\"Check partial rsqs values of feature corresponding confounding amonunt of ATE\n",
    "        Args:\n",
    "            sens_df (pandas.DataFrame): a data frame output from causalsens\n",
    "            feature_name (str): feature name to check\n",
    "            partial_rsqs_value (float) : partial rsquare value of feature\n",
    "            range (float) : range to search from sens_df\n",
    "\n",
    "        Return: min and max value of confounding amount\n",
    "        \"\"\"\n",
    "\n",
    "        rsqs_dict = []\n",
    "        for i in sens_df.rsqs:\n",
    "            if partial_rsqs_value - partial_rsqs_value*range < i < partial_rsqs_value + partial_rsqs_value*range:\n",
    "                rsqs_dict.append(i)\n",
    "\n",
    "        if rsqs_dict:\n",
    "            confounding_min = sens_df[sens_df.rsqs.isin(rsqs_dict)].alpha.min()\n",
    "            confounding_max = sens_df[sens_df.rsqs.isin(rsqs_dict)].alpha.max()\n",
    "            logger.info('Only works for linear outcome models right now. Check back soon.')\n",
    "            logger.info('For feature {} with partial rsquare {} confounding amount with possible values: {}, {}'.format(\n",
    "                        feature_name, partial_rsqs_value, confounding_min, confounding_max))\n",
    "            return [confounding_min, confounding_max]\n",
    "        else:\n",
    "            logger.info('Cannot find correponding rsquare value within the range for input, please edit confounding', 'values vector or use a larger range and try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted 02_analyzers.ipynb.\n",
      "Converted 03_key_driver_analysis.ipynb.\n",
      "Converted 04_preprocessing.ipynb.\n",
      "Converted 05a_meta.base.ipynb.\n",
      "Converted 05b_meta.tleaerner.ipynb.\n",
      "Converted 05c_meta.slearner.ipynb.\n",
      "Converted 05d_meta.xlearner.ipynb.\n",
      "Converted 05e_meta.rlearner.ipynb.\n",
      "Converted 05f_meta.utils.ipynb.\n",
      "Converted 05g_meta.explainer.ipynb.\n",
      "Converted 05h_meta.propensity.ipynb.\n",
      "Converted 05i_meta.sensitivity.ipynb.\n",
      "Converted 99_examples.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
