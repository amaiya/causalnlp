{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp causalinference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference\n",
    "\n",
    "> Causal Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import time\n",
    "from causalml.inference.meta import BaseTClassifier, BaseXClassifier, BaseRClassifier\n",
    "from causalml.inference.meta import BaseTRegressor, BaseXRegressor, BaseRRegressor\n",
    "from scipy import stats\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.match import NearestNeighborMatch, create_table_one\n",
    "import pandas as pd\n",
    "\n",
    "metalearner_cls_dict = {'t-learner' : BaseTClassifier,\n",
    "                        'x-learner' : BaseXClassifier,\n",
    "                        'r-learner' : BaseRClassifier}\n",
    "metalearner_reg_dict = {'t-learner' : BaseTRegressor,\n",
    "                        'x-learner' : BaseXRegressor,\n",
    "                        'r-learner' : BaseRRegressor}\n",
    "\n",
    "class CausalModel:\n",
    "    \"\"\"\n",
    "    Infers causality from the data contained in `df` using a metalearner.\n",
    "    The `treat_col` column should contain binary values: 1 for treated, 0 for untreated.\n",
    "    The `outcome_col` column should contain the outcome values, which can be either numeric (ints or floats)\n",
    "    or categorical (strings).\n",
    "    The `text_col` column contains the text values (e.g., articles, reviews, emails).\n",
    "    All other columns are treated as additional numerical or categorical covariates unless\n",
    "    they appear in `ignore_cols`.   \n",
    "    The `learner` parameter can be used to supply a custom learner to the metalearner.\n",
    "    Example: `learner = LGBMClassifier(n_estimators=1000)`\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 treatment_col='treatment', \n",
    "                 outcome_col='outcome', \n",
    "                 text_col='text',\n",
    "                 ignore_cols=[],\n",
    "                 learner = None,\n",
    "                 treatment_effect_col = 'treatment_effect',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        \"\"\"\n",
    "\n",
    "        self.treatment_col = treatment_col\n",
    "        self.outcome_col = outcome_col\n",
    "        self.text_col = text_col # currently ignored\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.te = treatment_effect_col\n",
    "        self.v = verbose\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # these are auto-populated by preprocess method\n",
    "        self.is_classification = True       \n",
    "        self.feature_names = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.treatment = None\n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocess(self.df)\n",
    "\n",
    "        # setup model\n",
    "        metalearner_type = 't-learner' # support T-Learners for now\n",
    "        if self.is_classification:\n",
    "            learner = LGBMClassifier() if learner is None else learner\n",
    "            metalearner_cls = metalearner_cls_dict[metalearner_type]              \n",
    "        else:\n",
    "            learner = LGBMRegressor() if learner is None else learner\n",
    "            metalearner_cls = metalearner_reg_dict[metalearner_type]\n",
    "        if metalearner_cls in [BaseTClassifier, BaseTRegressor]:\n",
    "            self.model = metalearner_cls(learner=learner,control_name=0)\n",
    "        else:\n",
    "            self.model = metalearner_cls(outcome_learner=learner,\n",
    "                                     effect_learner=learner,\n",
    "                                     control_name=0) \n",
    "           \n",
    "\n",
    "    def preprocess(self, df=None, na_cont_value=-1, na_cat_value='MISSING'):\n",
    "        \"\"\"\n",
    "        Preprocess a dataframe for causal inference.\n",
    "        If df is None, uses self.df.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # step 1: check/clean dataframe\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError('df must be a pandas DataFrame')\n",
    "        df = df.rename(columns=lambda x: x.strip()) # strip headers \n",
    "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)  # strip data\n",
    "        df, _ = self._preprocess_column(df, self.treatment_col, is_treatment=True)\n",
    "        df, self.is_classification = self._preprocess_column(df, self.outcome_col, is_treatment=False)\n",
    "        self.feature_names = [c for c in df.columns.values \\\n",
    "                             if c not in [self.treatment_col, self.outcome_col]+self.ignore_cols]\n",
    "        self.x = df[self.feature_names].copy()\n",
    "        self.y = df[self.outcome_col].copy()\n",
    "        self.treatment = df[self.treatment_col].copy()\n",
    "\n",
    "        # step 2: fill empty values on x\n",
    "        for c in self.feature_names:\n",
    "            if self._check_type(df, c)['dtype'] =='string': self.x[c] = self.x[c].fillna(na_cat_value)\n",
    "            if self._check_type(df, c)['dtype']=='numeric': self.x[c] = self.x[c].fillna(na_cont_value)\n",
    "\n",
    "        # step 3: one-hot encode categorial features\n",
    "        for c in self.feature_names:\n",
    "            if self._check_type(df, c)['dtype']=='string':\n",
    "                self.x = self.x.merge(pd.get_dummies(self.x[c], prefix = c, drop_first=True), left_index=True, right_index=True)\n",
    "                del self.x[c]\n",
    "        self.feature_names_one_hot = self.x.columns\n",
    "        if self.v: print('outcome is: %s' % ('categorical' if self.is_classification else 'numerical'))\n",
    "        if self.v: print(\"preprocess time: \", -start_time + time.time(),\" sec\")\n",
    "\n",
    "        return df\n",
    "        \n",
    "        \n",
    "    def _preprocess_column(self, df, col, is_treatment=True):\n",
    "        \"\"\"\n",
    "        Preprocess treatment and outcome columns.\n",
    "        \"\"\"\n",
    "        # remove nulls\n",
    "        df = df[df[col].notnull()]\n",
    "\n",
    "        # check if already binarized\n",
    "        if self._check_binary(df, col): return df, True\n",
    "\n",
    "        # inspect column\n",
    "        d = self._check_type(df, col)\n",
    "        typ = d['dtype']\n",
    "        num = d['nunique']\n",
    "        \n",
    "        # process as treatment\n",
    "        if is_treatment:\n",
    "            if typ == 'numeric' or (typ == 'string' and num != 2): \n",
    "                raise ValueError('Treatment column must contain only two unique values ' +\\\n",
    "                                 'indicating the treated and control groups.')\n",
    "            values = sorted(df[col].unique())\n",
    "            df[col].replace(values, [0,1], inplace=True)\n",
    "            if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        # process as outcome\n",
    "        else:\n",
    "            if typ == 'string' and num != 2:\n",
    "                raise ValueError('If the outcome column is string/categorical, it must '+\n",
    "                                'contain only two unique values.')\n",
    "            if typ == 'string':\n",
    "                values = sorted(df[col].unique())\n",
    "                df[col].replace(values, [0,1], inplace=True)\n",
    "                if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        return df, self._check_binary(df, col)\n",
    "        \n",
    "        \n",
    "    def _check_type(self, df, col):\n",
    "        from pandas.api.types import is_string_dtype\n",
    "        from pandas.api.types import is_numeric_dtype\n",
    "        dtype = None\n",
    "        \n",
    "        tmp_var = df[df[col].notnull()][col]\n",
    "        #if tmp_var.nunique()<=5: return 'cat'\n",
    "        if is_numeric_dtype(tmp_var): dtype = 'numeric'\n",
    "        elif is_string_dtype(tmp_var): dtype =  'string'\n",
    "        else:\n",
    "            raise ValueError('Columns in dataframe must be either numeric or strings.  ' +\\\n",
    "                             'Column %s is neither' % (col))\n",
    "        output = {'dtype' : dtype, 'nunique' : tmp_var.nunique()}\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def _check_binary(self, df, col):\n",
    "        return df[col].isin([0,1]).all()        \n",
    "\n",
    "    def _get_feature_names(self, df):\n",
    "        return [c for c in df.columns.values \\\n",
    "                if c not in [self.treatment_col, self.outcome_col]+self.ignore_cols]\n",
    "    \n",
    "    def fit(self):\n",
    "        print(\"start fitting causal model\")\n",
    "        start_time = time.time()\n",
    "        self.model.fit(self.x.values, self.treatment.values, self.y.values)\n",
    "        preds = self.predict(self.x)\n",
    "        self.df[self.te] = preds\n",
    "        print(\"time to fit causalmodel: \",-start_time + time.time(),\" sec\")\n",
    "            \n",
    "    def predict(self, x):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            return self.model.predict(x.values)\n",
    "        else:\n",
    "            return self.model.predict(x)\n",
    "    \n",
    "    def estimate_ate(self, bool_mask=None):\n",
    "        df = self.df if bool_mask is None else self.df[bool_mask]\n",
    "        a = df[self.te].values\n",
    "        mean = np.mean(a)\n",
    "        return {'ate' : mean}\n",
    "        \n",
    "\n",
    "        \n",
    "    def minimize_bias(self, caliper = None):\n",
    "            print('-------Start bias minimization procedure----------')\n",
    "            start_time = time.time()\n",
    "            #Join x, y and treatment vectors\n",
    "            df_match = self.x.merge(self.treatment,left_index=True, right_index=True)\n",
    "            df_match = df_match.merge(self.y, left_index=True, right_index=True)\n",
    "\n",
    "            #buld propensity model. Propensity is the probability of raw belongs to control group.\n",
    "            pm = ElasticNetPropensityModel(n_fold=3, random_state=42)\n",
    "\n",
    "            #ps - propensity score\n",
    "            df_match['ps'] = pm.fit_predict(self.x, self.treatment)\n",
    "\n",
    "            #Matching model object\n",
    "            psm = NearestNeighborMatch(replace=False,\n",
    "                           ratio=1,\n",
    "                           random_state=423,\n",
    "                           caliper=caliper)\n",
    "\n",
    "            ps_cols = list(self.feature_names_one_hot)\n",
    "            ps_cols.append('ps')\n",
    "\n",
    "            #Apply matching model\n",
    "            #If error, then sample is unbiased and we don't do anything\n",
    "            self.flg_bias = True\n",
    "            self.df_unbiased = psm.match(data=df_match, treatment_col='treatment',score_cols=['ps'])\n",
    "            self.x_unbiased = self.df_unbiased[self.x.columns]\n",
    "            self.y_unbiased = self.df_unbiased[self.outcome_col]\n",
    "            self.treatment_unbiased = self.df_unbiased['treatment']\n",
    "            print('-------------------MATCHING RESULTS----------------')\n",
    "            print('-----BEFORE MATCHING-------')\n",
    "            print(create_table_one(data=df_match,\n",
    "                                    treatment_col='treatment',\n",
    "                                    features=list(self.feature_names_one_hot)))\n",
    "            print('-----AFTER MATCHING-------')\n",
    "            print(create_table_one(data=self.df_unbiased,\n",
    "                                    treatment_col='treatment',\n",
    "                                    features=list(self.feature_names_one_hot)))\n",
    "            return self.df_unbiased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Does a medical treatment help reduce kidney stones?\n",
    "\n",
    "We use the well-known [kidney stones example problem](https://en.wikipedia.org/wiki/Simpson%27s_paradox#Kidney_stone_treatment). Let's [create](https://github.com/uber/causalml/issues/249#issue-743803593) the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_Surgery?</th>\n",
       "      <th>Large_Stone?</th>\n",
       "      <th>Favorable?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Open_Surgery?  Large_Stone?  Favorable?\n",
       "0              0             0           0\n",
       "1              0             0           0\n",
       "2              0             0           0\n",
       "3              0             0           0\n",
       "4              0             0           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "data = ((*a, b) for (a, b) in zip(itertools.product([0,1], [0,1], [0,1]), [36, 234, 25, 55, 6, 81, 71, 192]))\n",
    "df = pd.DataFrame(data, columns=['Open_Surgery?', 'Large_Stone?', 'Favorable?', 'N'])\n",
    "df = df.loc[df.index.repeat(df['N'])].reset_index(drop=True).drop(columns=['N'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the `CausalModel` and train it with `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome is: categorical\n",
      "preprocess time:  0.010984182357788086  sec\n",
      "start fitting causal model\n",
      "time to fit causalmodel:  1.0818510055541992  sec\n"
     ]
    }
   ],
   "source": [
    "cm = CausalModel(df, treatment_col='Open_Surgery?', outcome_col='Favorable?', \n",
    "                      ignore_cols=[]) # ignore columns used as treatment (N/A  here)\n",
    "cm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall average treatment is correctly estimated as 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.05366850622769351}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate = cm.estimate_ate()\n",
    "ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ate['ate'] > 0.05\n",
    "assert ate['ate'] < 0.055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The treatment effects for patients with larger kidney stones vs. smaller kidney stones can also be estimated. Those with larger kidney stones, which are harder-to-treat, are correctly estimated to have a lower treatment effect than those with smaller kindney stones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.042535751074149745}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(cm.df['Large_Stone?']==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.06436468274776497}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(cm.df['Large_Stone?']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
