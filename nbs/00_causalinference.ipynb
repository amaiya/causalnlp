{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp causalinference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference\n",
    "\n",
    "> Causal Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import time\n",
    "from causalml.inference.meta import BaseTClassifier, BaseXClassifier, BaseRClassifier, BaseSClassifier\n",
    "from causalml.inference.meta import BaseTRegressor, BaseXRegressor, BaseRRegressor, BaseSRegressor\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.match import NearestNeighborMatch, create_table_one\n",
    "from scipy import stats\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "# from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "\n",
    "\n",
    "\n",
    "metalearner_cls_dict = {'t-learner' : BaseTClassifier,\n",
    "                        'x-learner' : BaseXClassifier,\n",
    "                        'r-learner' : BaseRClassifier,\n",
    "                         's-learner': BaseSClassifier}\n",
    "metalearner_reg_dict = {'t-learner' : BaseTRegressor,\n",
    "                        'x-learner' : BaseXRegressor,\n",
    "                        'r-learner' : BaseRRegressor,\n",
    "                        's-learner' : BaseSRegressor}\n",
    "\n",
    "class CausalInferenceModel:\n",
    "    \"\"\"Infers causality from the data contained in `df` using a metalearner.\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> cm = CausalInferenceModel(df, \n",
    "                                  treatment_col='Is_Male?', \n",
    "                                  outcome_col='Post_Shared?', text_col='Post_Text',\n",
    "                                  ignore_cols=['id', 'email'])\n",
    "        cm.fit()\n",
    "    ```\n",
    "    \n",
    "    **Parameters:**\n",
    "    \n",
    "    * **df** : pandas.DataFrame containing dataset\n",
    "    * **treatment_col** : treatment variable; column should contain binary values: 1 for treated, 0 for untreated.\n",
    "    * **outcome_col** : outcome variable; column should contain the categorical or numeric outcome values\n",
    "    * **text_col** : (optional) text column containing the strings (e.g., articles, reviews, emails). \n",
    "    * **ignore_cols** : columns to ignore in the analysis\n",
    "    * **include_cols** : columns to include as covariates (e.g., possible confounders)\n",
    "    * **treatment_effect_col** : name of column to hold causal effect estimations.  Does not need to exist.  Created by CausalNLP.\n",
    "    * **metalearner_type** : metalearner model to use. One of {'t-learner', 's-learner', 'x-learner', 'r-learner'} (Default: 't-learner')\n",
    "    * **learner** : an instance of a custom learner.  If None, a default LightGBM will be used.\n",
    "        # Example\n",
    "         learner = LGBMClassifier(n_estimators=1000)\n",
    "    * **min_df** : min_df parameter used for text processing using sklearn\n",
    "    * **max_df** : max_df parameter used for text procesing using sklearn\n",
    "    * **stop_words** : stop words used for text processing (from sklearn)\n",
    "    * **verbose** : If 1, print informational messages.  If 0, suppress.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 treatment_col='treatment', \n",
    "                 outcome_col='outcome', \n",
    "                 text_col=None,\n",
    "                 ignore_cols=[],\n",
    "                 include_cols=[],\n",
    "                 treatment_effect_col = 'treatment_effect',\n",
    "                 metalearner_type='t-learner',\n",
    "                 learner = None,\n",
    "                 min_df=0.05,\n",
    "                 max_df=0.5,\n",
    "                 stop_words='english',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        \"\"\"\n",
    "\n",
    "        self.treatment_col = treatment_col\n",
    "        self.outcome_col = outcome_col\n",
    "        self.text_col = text_col\n",
    "        self.te = treatment_effect_col # created\n",
    "        self.metalearner_type = metalearner_type\n",
    "        self.v = verbose\n",
    "        self.df = df.copy()\n",
    "        self.min_df = 0.05\n",
    "        self.max_df = 0.5\n",
    "        self.stop_words = stop_words\n",
    "        if not isinstance(ignore_cols, list):\n",
    "            raise ValueError('ignore_cols must be a list.')\n",
    "        if not isinstance(include_cols, list):\n",
    "            raise ValueError('include_cols must be a list.')\n",
    "        if ignore_cols and include_cols:\n",
    "            raise  ValueError('ignore_cols and include_cols are mutually exclusive.  Please choose one.')\n",
    "        if include_cols:\n",
    "            ignore_cols = [c for c in df.columns.values if c not in include_cols + [treatment_col, \n",
    "                                                                                    outcome_col, \n",
    "                                                                                    text_col]]\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.include_cols = include_cols\n",
    "            \n",
    "        if text_col is not None and text_col not in df:\n",
    "            raise ValueError(f'You specified text_col=\"{text_col}\", but {text_col} is not a column in df.')\n",
    "        if self.treatment_col in self.ignore_cols:\n",
    "            raise ValueError(f'ignore_cols contains the treatment column ({treatment_col})')\n",
    "        if self.outcome_col in self.ignore_cols:\n",
    "            raise ValueError(f'ignore_cols contains the outcome column ({outcome_col})')\n",
    "            \n",
    "        \n",
    "        # these are auto-populated by preprocess method\n",
    "        self.is_classification = True       \n",
    "        self.feature_names = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.treatment = None\n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocess(self.df)\n",
    "\n",
    "        # setup model\n",
    "        self.model = self._create_metalearner(metalearner_type=self.metalearner_type,\n",
    "                                             supplied_learner=learner)\n",
    "\n",
    "           \n",
    "\n",
    "    def _create_metalearner(self, metalearner_type='t-learner', supplied_learner=None):\n",
    "        # set learner\n",
    "        default_learner = None\n",
    "        if metalearner_type == 's-learner':\n",
    "            if self.is_classification:\n",
    "                default_learner =  LogisticRegression()\n",
    "            else:\n",
    "                default_learner = LinearRegression()\n",
    "        else:\n",
    "            if self.is_classification:\n",
    "                default_learner = LGBMClassifier()\n",
    "            else:\n",
    "                default_learner =  LGBMRegressor()\n",
    "        learner = default_learner if supplied_learner is None else supplied_learner\n",
    "        \n",
    "        # set metalearner\n",
    "        metalearner_class = metalearner_cls_dict[metalearner_type] if self.is_classification \\\n",
    "                                                                   else metalearner_reg_dict[metalearner_type]\n",
    "        if metalearner_type in ['t-learner', 's-learner']:\n",
    "            model = metalearner_class(learner=learner,control_name=0)\n",
    "        else:\n",
    "            model = metalearner_class(outcome_learner=learner,\n",
    "                                      effect_learner=LGBMRegressor(),\n",
    "                                      control_name=0) \n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def preprocess(self, df=None, na_cont_value=-1, na_cat_value='MISSING'):\n",
    "        \"\"\"\n",
    "        Preprocess a dataframe for causal inference.\n",
    "        If df is None, uses self.df.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # step 1: check/clean dataframe\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError('df must be a pandas DataFrame')\n",
    "        df = df.rename(columns=lambda x: x.strip()) # strip headers \n",
    "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)  # strip data\n",
    "        df, _ = self._preprocess_column(df, self.treatment_col, is_treatment=True)\n",
    "        df, self.is_classification = self._preprocess_column(df, self.outcome_col, is_treatment=False)\n",
    "        self.feature_names = [c for c in df.columns.values \\\n",
    "                             if c not in [self.treatment_col, \n",
    "                                          self.outcome_col, self.text_col]+self.ignore_cols]\n",
    "        #if self.text_col is not None and self.text_col in self.feature_names:\n",
    "        #    warnings.warn(f'Since you specified text_col=\"{self.text_col}\", other columns that are not ' +\\\n",
    "        #                  'treatments or outcomes are ignored and the metalearner will use ' +\\\n",
    "        #                  'a text classifier/regressor as the base learner.') \n",
    "        #    self.feature_names = [self.text_col]\n",
    "        self.x = df[self.feature_names].copy()\n",
    "        self.y = df[self.outcome_col].copy()\n",
    "        self.treatment = df[self.treatment_col].copy()\n",
    "    \n",
    "\n",
    "        # step 2: fill empty values on x\n",
    "        for c in self.feature_names:\n",
    "            if self._check_type(df, c)['dtype'] =='string': self.x[c] = self.x[c].fillna(na_cat_value)\n",
    "            if self._check_type(df, c)['dtype']=='numeric': self.x[c] = self.x[c].fillna(na_cont_value)\n",
    "           \n",
    "\n",
    "        # step 3: one-hot encode categorial features\n",
    "        for c in self.feature_names:\n",
    "            if c == self.text_col: continue\n",
    "            if self._check_type(df, c)['dtype']=='string':\n",
    "                if self.df[c].nunique()/self.df.shape[0] > 0.5:\n",
    "                    if self.text_col is not None:\n",
    "                        err_msg = f'Column \"{c}\" looks like it contains free-form text. ' +\\\n",
    "                        f'Since there is already a text_col specified ({self.text_col}), '+\\\n",
    "                        f'you should probably include this column in the \"ignore_cols\" list.'\n",
    "                    else:\n",
    "                        err_msg = f'Column \"{c}\" looks like it contains free-form text. ' +\\\n",
    "                        f'Please either set text_col=\"{c}\" or add it to \"ignore_cols\" list.'\n",
    "                    raise ValueError(err_msg)\n",
    "                        \n",
    "                self.x = self.x.merge(pd.get_dummies(self.x[c], prefix = c, \n",
    "                                                     drop_first=False), \n",
    "                                                     left_index=True, right_index=True)\n",
    "                del self.x[c]\n",
    "        self.feature_names_one_hot = self.x.columns\n",
    "        \n",
    "                        \n",
    "        # step 4: for text-based confounder, use extracted vocabulary as features\n",
    "        if self.text_col is not None:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            tv = TfidfVectorizer(min_df=self.min_df, max_df=self.max_df, stop_words=self.stop_words)\n",
    "            v_features = tv.fit_transform(self.df[self.text_col])\n",
    "            vocab = tv.get_feature_names()\n",
    "            vocab_df = pd.DataFrame(v_features.toarray(), columns = [\"v_%s\" % (v) for v in vocab])\n",
    "            self.x = pd.concat([self.x, vocab_df], axis=1, join='inner')\n",
    "        outcome_type = 'categorical' if self.is_classification else 'numerical'\n",
    "        if self.v: print(f'outcome column ({outcome_type}): {self.outcome_col}')\n",
    "        if self.v: print(f'treatment column: {self.treatment_col}')\n",
    "        if self.v: print('numerical/categorical covariates: %s' % (self.feature_names))\n",
    "        if self.v and self.text_col: print('text covariate: %s' % (self.text_col))\n",
    "        if self.v: print(\"preprocess time: \", -start_time + time.time(),\" sec\")\n",
    "\n",
    "        return df\n",
    "        \n",
    "        \n",
    "    def _preprocess_column(self, df, col, is_treatment=True):\n",
    "        \"\"\"\n",
    "        Preprocess treatment and outcome columns.\n",
    "        \"\"\"\n",
    "        # remove nulls\n",
    "        df = df[df[col].notnull()]\n",
    "\n",
    "        # check if already binarized\n",
    "        if self._check_binary(df, col): return df, True\n",
    "\n",
    "        # inspect column\n",
    "        d = self._check_type(df, col)\n",
    "        typ = d['dtype']\n",
    "        num = d['nunique']\n",
    "        \n",
    "        # process as treatment\n",
    "        if is_treatment:\n",
    "            if typ == 'numeric' or (typ == 'string' and num != 2): \n",
    "                raise ValueError('Treatment column must contain only two unique values ' +\\\n",
    "                                 'indicating the treated and control groups.')\n",
    "            values = sorted(df[col].unique())\n",
    "            df[col].replace(values, [0,1], inplace=True)\n",
    "            if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        # process as outcome\n",
    "        else:\n",
    "            if typ == 'string' and num != 2:\n",
    "                raise ValueError('If the outcome column is string/categorical, it must '+\n",
    "                                'contain only two unique values.')\n",
    "            if typ == 'string':\n",
    "                values = sorted(df[col].unique())\n",
    "                df[col].replace(values, [0,1], inplace=True)\n",
    "                if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        return df, self._check_binary(df, col)\n",
    "        \n",
    "        \n",
    "    def _check_type(self, df, col):\n",
    "        from pandas.api.types import is_string_dtype\n",
    "        from pandas.api.types import is_numeric_dtype\n",
    "        dtype = None\n",
    "        \n",
    "        tmp_var = df[df[col].notnull()][col]\n",
    "        #if tmp_var.nunique()<=5: return 'cat'\n",
    "        if is_numeric_dtype(tmp_var): dtype = 'numeric'\n",
    "        elif is_string_dtype(tmp_var): dtype =  'string'\n",
    "        else:\n",
    "            raise ValueError('Columns in dataframe must be either numeric or strings.  ' +\\\n",
    "                             'Column %s is neither' % (col))\n",
    "        output = {'dtype' : dtype, 'nunique' : tmp_var.nunique()}\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def _check_binary(self, df, col):\n",
    "        return df[col].isin([0,1]).all()        \n",
    "\n",
    "    def _get_feature_names(self, df):\n",
    "        return [c for c in df.columns.values \\\n",
    "                if c not in [self.treatment_col, self.outcome_col]+self.ignore_cols]\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fits a causal inference model and estimates outcome\n",
    "        with and without treatment for each observation.\n",
    "        \"\"\"\n",
    "        print(\"start fitting causal inference model\")\n",
    "        start_time = time.time()\n",
    "        self.model.fit(self.x.values, self.treatment.values, self.y.values)\n",
    "        preds = self.predict(self.x)\n",
    "        self.df[self.te] = preds\n",
    "        print(\"time to fit causal inference model: \",-start_time + time.time(),\" sec\")\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Estimates the treatment effect for each observation in `x`.\n",
    "        \"\"\"\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            return self.model.predict(x.values)\n",
    "        else:\n",
    "            return self.model.predict(x)\n",
    "    \n",
    "    def estimate_ate(self, bool_mask=None):\n",
    "        \"\"\"\n",
    "        Estimates the treatment effect for each observation in\n",
    "        `self.df`.\n",
    "        \"\"\"\n",
    "        df = self.df if bool_mask is None else self.df[bool_mask]\n",
    "        a = df[self.te].values\n",
    "        mean = np.mean(a)\n",
    "        return {'ate' : mean}\n",
    "        \n",
    "\n",
    "        \n",
    "    def minimize_bias(self, caliper = None):\n",
    "            print('-------Start bias minimization procedure----------')\n",
    "            start_time = time.time()\n",
    "            #Join x, y and treatment vectors\n",
    "            df_match = self.x.merge(self.treatment,left_index=True, right_index=True)\n",
    "            df_match = df_match.merge(self.y, left_index=True, right_index=True)\n",
    "\n",
    "            #buld propensity model. Propensity is the probability of raw belongs to control group.\n",
    "            pm = ElasticNetPropensityModel(n_fold=3, random_state=42)\n",
    "\n",
    "            #ps - propensity score\n",
    "            df_match['ps'] = pm.fit_predict(self.x, self.treatment)\n",
    "\n",
    "            #Matching model object\n",
    "            psm = NearestNeighborMatch(replace=False,\n",
    "                           ratio=1,\n",
    "                           random_state=423,\n",
    "                           caliper=caliper)\n",
    "\n",
    "            ps_cols = list(self.feature_names_one_hot)\n",
    "            ps_cols.append('ps')\n",
    "\n",
    "            #Apply matching model\n",
    "            #If error, then sample is unbiased and we don't do anything\n",
    "            self.flg_bias = True\n",
    "            self.df_unbiased = psm.match(data=df_match, treatment_col='treatment',score_cols=['ps'])\n",
    "            self.x_unbiased = self.df_unbiased[self.x.columns]\n",
    "            self.y_unbiased = self.df_unbiased[self.outcome_col]\n",
    "            self.treatment_unbiased = self.df_unbiased['treatment']\n",
    "            print('-------------------MATCHING RESULTS----------------')\n",
    "            print('-----BEFORE MATCHING-------')\n",
    "            print(create_table_one(data=df_match,\n",
    "                                    treatment_col='treatment',\n",
    "                                    features=list(self.feature_names_one_hot)))\n",
    "            print('-----AFTER MATCHING-------')\n",
    "            print(create_table_one(data=self.df_unbiased,\n",
    "                                    treatment_col='treatment',\n",
    "                                    features=list(self.feature_names_one_hot)))\n",
    "            return self.df_unbiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CausalInferenceModel.fit\" class=\"doc_header\"><code>CausalInferenceModel.fit</code><a href=\"__main__.py#L284\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CausalInferenceModel.fit</code>()\n",
       "\n",
       "Fits a causal inference model and estimates outcome\n",
       "with and without treatment for each observation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CausalInferenceModel.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CausalInferenceModel.predict\" class=\"doc_header\"><code>CausalInferenceModel.predict</code><a href=\"__main__.py#L296\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CausalInferenceModel.predict</code>(**`x`**)\n",
       "\n",
       "Estimates the treatment effect for each observation in `x`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CausalInferenceModel.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `x` should be either a `pandas.DataFrame` or a `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CausalInferenceModel.estimate_ate\" class=\"doc_header\"><code>CausalInferenceModel.estimate_ate</code><a href=\"__main__.py#L305\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CausalInferenceModel.estimate_ate</code>(**`bool_mask`**=*`None`*)\n",
       "\n",
       "Estimates the treatment effect for each observation in\n",
       "`self.df`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CausalInferenceModel.estimate_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bool_mask` parameter can be used to estimate the conditional average treatment estimate (CATE).\n",
    "For instance, to estimate the average treatment effect for only those individuals over 18 years of age:\n",
    "\n",
    "```python\n",
    "cm.estimate_ate(cm.df['age']>18])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Usage Example: Do social media posts by women get shared more often than those by men?\n",
    "\n",
    "Let's create a simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Male?</th>\n",
       "      <th>Post_Text</th>\n",
       "      <th>Post_Shared?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Is_Male?              Post_Text  Post_Shared?\n",
       "0         0  I really love my job!             0\n",
       "1         0  I really love my job!             0\n",
       "2         0  I really love my job!             0\n",
       "3         0  I really love my job!             0\n",
       "4         0  I really love my job!             0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "data = ((*a, b) for (a, b) in zip(itertools.product([0,1], [0,1], [0,1]), [36, 234, 25, 55, 6, 81, 71, 192]))\n",
    "df = pd.DataFrame(data, columns=['Is_Male?', 'Post_Text', 'Post_Shared?', 'N'])\n",
    "df = df.loc[df.index.repeat(df['N'])].reset_index(drop=True).drop(columns=['N'])\n",
    "values = sorted(df['Post_Text'].unique())\n",
    "df['Post_Text'].replace(values, ['I really love my job!', 'My boss is pretty terrible.'], inplace=True)\n",
    "original_df = df.copy()\n",
    "df = None\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it seems like posts by women get shared more often.  More specifically, it appears that being male **reduces** your the chance your post is shared by 4.5 percentage points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_probability = original_df[(original_df['Is_Male?']==1)]['Post_Shared?'].value_counts(normalize=True)[1]\n",
    "male_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8257142857142857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_probability = original_df[(original_df['Is_Male?']==0)]['Post_Shared?'].value_counts(normalize=True)[1]\n",
    "female_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04571428571428571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_probability-female_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is inaccurate. In fact, this is an example of [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox), and the true causal effect of being male in this simulated datsaet is roughly **0.05** (as opposed to **-0.045**) with men's posts being more likely to be shared. The reason is that women in this simulation tend to make more positive posts which tend to be shared more often here. Post sentiment, then, is a [mediator](https://en.wikipedia.org/wiki/Mediation_(statistics), which is statistically the same thing as a [confounder](https://en.wikipedia.org/wiki/Confounding).   \n",
    "\n",
    "When controlling for the sentiment of the post (the mediator variable in this dataset), it is revealed that men's posts are, in fact, shared more often (for both negative posts and positive posts). This can be quickly and easily estimated in **CausalNLP**.\n",
    "\n",
    "### Causal Inference from Text with Autocoders\n",
    "\n",
    "Let's first use the `Autocoder` to transform the raw text into sentiment.  We can then control for sentiment when estimating the causal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnlp.autocoder import Autocoder\n",
    "ac = Autocoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ac.code_sentiment(original_df['Post_Text'].values, original_df, binarize=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Male?</th>\n",
       "      <th>Post_Text</th>\n",
       "      <th>Post_Shared?</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.980809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.980809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.980809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.980809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.980809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Is_Male?              Post_Text  Post_Shared?  negative  positive\n",
       "0         0  I really love my job!             0  0.019191  0.980809\n",
       "1         0  I really love my job!             0  0.019191  0.980809\n",
       "2         0  I really love my job!             0  0.019191  0.980809\n",
       "3         0  I really love my job!             0  0.019191  0.980809\n",
       "4         0  I really love my job!             0  0.019191  0.980809"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When autocoding the raw text for sentiment, we have chosen to binarize the probabilities (`binarize=True`).  Raw probabilities can also be used with `binarize=False`.\n",
    "\n",
    "Next, let's estimate the treatment effects. We will ignore the `positive` and `Post_Shared?` columns, as their information is captured by the `negative` column in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome column (categorical): Post_Shared?\n",
      "treatment column: Is_Male?\n",
      "numerical/categorical covariates: ['negative']\n",
      "preprocess time:  0.014408588409423828  sec\n",
      "start fitting causal inference model\n",
      "time to fit causal inference model:  0.5292856693267822  sec\n"
     ]
    }
   ],
   "source": [
    "cm = CausalInferenceModel(df, treatment_col='Is_Male?', outcome_col='Post_Shared?',\n",
    "                          include_cols=['negative'])\n",
    "cm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon controlling for sentiment, we see that the overall average treatment is correctly estimated as 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.05366850622769351}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate = cm.estimate_ate()\n",
    "ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CausalNLP** allows you to easily compute conditional or individualized treatment effects.\n",
    "For instance, for negative posts, being male increases the chance of your post being shared by about 4 percentage points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.042535751074149745}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(cm.df['negative']>0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive posts, being male increases the chance of your post being shared by about 6 percentage points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.06436468274776497}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(cm.df['negative']<0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ate['ate'] > 0.05\n",
    "assert ate['ate'] < 0.055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Inference Using Raw Text as a Confounder/Mediator\n",
    "\n",
    "In the example above, we approached the problem under the assumption that a specific lingustic property (sentiment) was an important mediator or confounder for which to control. In some cases, there may also be other unknown lingustic properties that are potential confounders/mediators (e.g., topic, politeness, toxic language, readability).  \n",
    "\n",
    "In **CausalNLP**, we can also use the **raw text** as the potential confounder/mediator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome column (categorical): Post_Shared?\n",
      "treatment column: Is_Male?\n",
      "numerical/categorical covariates: []\n",
      "text covariate: Post_Text\n",
      "preprocess time:  0.01554560661315918  sec\n",
      "start fitting causal inference model\n",
      "time to fit causal inference model:  0.06210470199584961  sec\n"
     ]
    }
   ],
   "source": [
    "cm = CausalInferenceModel(df, treatment_col='Is_Male?', outcome_col='Post_Shared?', text_col='Post_Text',\n",
    "                         ignore_cols=['negative', 'positive'])\n",
    "cm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have excluded the **negative** and **positive** columns as extra covariates, you can use traditional categorical/numerical covariates in combination with a text field covariate (if they exist as extra columns in the dataframe).\n",
    "\n",
    "Here, we see that the same causal estimates are returned, as the text is easy to infer as positive or negative based on their correlations with the outcomes in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.05366850622769351}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate = cm.estimate_ate()\n",
    "ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.042535751074149745}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(df['Post_Text'] == 'My boss is pretty terrible.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.06436468274776497}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(df['Post_Text'] == 'I really love my job!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ate['ate'] > 0.05\n",
    "assert ate['ate'] < 0.055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Inference With Text as a Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we were interested in estimating the causal impact of **sentiment** on the outcome.  That is, **sentiment** of text is the treatment, and the **gender** is a potential confounder. As we did above, we can use the `Autocoder` to create the treatment variable.  The only difference is that we would supply the `binarize=True` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ac.code_sentiment(original_df['Post_Text'].values, original_df, binarize=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Male?</th>\n",
       "      <th>Post_Text</th>\n",
       "      <th>Post_Shared?</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Is_Male?              Post_Text  Post_Shared?  negative  positive\n",
       "0         0  I really love my job!             0         0         1\n",
       "1         0  I really love my job!             0         0         1\n",
       "2         0  I really love my job!             0         0         1\n",
       "3         0  I really love my job!             0         0         1\n",
       "4         0  I really love my job!             0         0         1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome column (categorical): Post_Shared?\n",
      "treatment column: positive\n",
      "numerical/categorical covariates: ['Is_Male?']\n",
      "preprocess time:  0.009786844253540039  sec\n",
      "start fitting causal inference model\n",
      "time to fit causal inference model:  0.6526179313659668  sec\n"
     ]
    }
   ],
   "source": [
    "cm = CausalInferenceModel(df, treatment_col='positive', outcome_col='Post_Shared?',\n",
    "                          include_cols=['Is_Male?'])\n",
    "cm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.19008080596986368}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate = cm.estimate_ate()\n",
    "ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ate['ate'] > 0.18\n",
    "assert ate['ate'] < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted examples.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
