{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp causalinference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference\n",
    "\n",
    "> Causal Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import time\n",
    "from causalml.inference.meta import BaseTClassifier, BaseXClassifier, BaseRClassifier\n",
    "from causalml.inference.meta import BaseTRegressor, BaseXRegressor, BaseRRegressor\n",
    "from scipy import stats\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.match import NearestNeighborMatch, create_table_one\n",
    "import pandas as pd\n",
    "\n",
    "metalearner_cls_dict = {'t-learner' : BaseTClassifier,\n",
    "                        'x-learner' : BaseXClassifier,\n",
    "                        'r-learner' : BaseRClassifier}\n",
    "metalearner_reg_dict = {'t-learner' : BaseTRegressor,\n",
    "                        'x-learner' : BaseXRegressor,\n",
    "                        'r-learner' : BaseRRegressor}\n",
    "\n",
    "class CausalInferenceModel:\n",
    "    \"\"\"\n",
    "    Infers causality from the data contained in `df` using a metalearner.\n",
    "    The `treat_col` column should contain binary values: 1 for treated, 0 for untreated.\n",
    "    The `outcome_col` column should contain the outcome values, which can be either numeric (ints or floats)\n",
    "    or categorical (strings).\n",
    "    The `text_col` column contains the text values (e.g., articles, reviews, emails).\n",
    "    All other columns are treated as additional numerical or categorical covariates unless\n",
    "    they appear in `ignore_cols`.   \n",
    "    The `learner` parameter can be used to supply a custom learner to the metalearner.\n",
    "    Example: `learner = LGBMClassifier(n_estimators=1000)`\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 treatment_col='treatment', \n",
    "                 outcome_col='outcome', \n",
    "                 text_col='text',\n",
    "                 ignore_cols=[],\n",
    "                 learner = None,\n",
    "                 treatment_effect_col = 'treatment_effect',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        \"\"\"\n",
    "\n",
    "        self.treatment_col = treatment_col\n",
    "        self.outcome_col = outcome_col\n",
    "        self.text_col = text_col # currently ignored\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.te = treatment_effect_col\n",
    "        self.v = verbose\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # these are auto-populated by preprocess method\n",
    "        self.is_classification = True       \n",
    "        self.feature_names = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.treatment = None\n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocess(self.df)\n",
    "\n",
    "        # setup model\n",
    "        metalearner_type = 't-learner' # support T-Learners for now\n",
    "        if self.is_classification:\n",
    "            learner = LGBMClassifier() if learner is None else learner\n",
    "            metalearner_cls = metalearner_cls_dict[metalearner_type]              \n",
    "        else:\n",
    "            learner = LGBMRegressor() if learner is None else learner\n",
    "            metalearner_cls = metalearner_reg_dict[metalearner_type]\n",
    "        if metalearner_cls in [BaseTClassifier, BaseTRegressor]:\n",
    "            self.model = metalearner_cls(learner=learner,control_name=0)\n",
    "        else:\n",
    "            self.model = metalearner_cls(outcome_learner=learner,\n",
    "                                     effect_learner=learner,\n",
    "                                     control_name=0) \n",
    "           \n",
    "\n",
    "    def preprocess(self, df=None, na_cont_value=-1, na_cat_value='MISSING'):\n",
    "        \"\"\"\n",
    "        Preprocess a dataframe for causal inference.\n",
    "        If df is None, uses self.df.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # step 1: check/clean dataframe\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError('df must be a pandas DataFrame')\n",
    "        df = df.rename(columns=lambda x: x.strip()) # strip headers \n",
    "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)  # strip data\n",
    "        df, _ = self._preprocess_column(df, self.treatment_col, is_treatment=True)\n",
    "        df, self.is_classification = self._preprocess_column(df, self.outcome_col, is_treatment=False)\n",
    "        self.feature_names = [c for c in df.columns.values \\\n",
    "                             if c not in [self.treatment_col, self.outcome_col]+self.ignore_cols]\n",
    "        self.x = df[self.feature_names].copy()\n",
    "        self.y = df[self.outcome_col].copy()\n",
    "        self.treatment = df[self.treatment_col].copy()\n",
    "\n",
    "        # step 2: fill empty values on x\n",
    "        for c in self.feature_names:\n",
    "            if self._check_type(df, c)['dtype'] =='string': self.x[c] = self.x[c].fillna(na_cat_value)\n",
    "            if self._check_type(df, c)['dtype']=='numeric': self.x[c] = self.x[c].fillna(na_cont_value)\n",
    "\n",
    "        # step 3: one-hot encode categorial features\n",
    "        for c in self.feature_names:\n",
    "            if self._check_type(df, c)['dtype']=='string':\n",
    "                self.x = self.x.merge(pd.get_dummies(self.x[c], prefix = c, drop_first=True), left_index=True, right_index=True)\n",
    "                del self.x[c]\n",
    "        self.feature_names_one_hot = self.x.columns\n",
    "        if self.v: print('outcome is: %s' % ('categorical' if self.is_classification else 'numerical'))\n",
    "        if self.v: print(\"preprocess time: \", -start_time + time.time(),\" sec\")\n",
    "\n",
    "        return df\n",
    "        \n",
    "        \n",
    "    def _preprocess_column(self, df, col, is_treatment=True):\n",
    "        \"\"\"\n",
    "        Preprocess treatment and outcome columns.\n",
    "        \"\"\"\n",
    "        # remove nulls\n",
    "        df = df[df[col].notnull()]\n",
    "\n",
    "        # check if already binarized\n",
    "        if self._check_binary(df, col): return df, True\n",
    "\n",
    "        # inspect column\n",
    "        d = self._check_type(df, col)\n",
    "        typ = d['dtype']\n",
    "        num = d['nunique']\n",
    "        \n",
    "        # process as treatment\n",
    "        if is_treatment:\n",
    "            if typ == 'numeric' or (typ == 'string' and num != 2): \n",
    "                raise ValueError('Treatment column must contain only two unique values ' +\\\n",
    "                                 'indicating the treated and control groups.')\n",
    "            values = sorted(df[col].unique())\n",
    "            df[col].replace(values, [0,1], inplace=True)\n",
    "            if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        # process as outcome\n",
    "        else:\n",
    "            if typ == 'string' and num != 2:\n",
    "                raise ValueError('If the outcome column is string/categorical, it must '+\n",
    "                                'contain only two unique values.')\n",
    "            if typ == 'string':\n",
    "                values = sorted(df[col].unique())\n",
    "                df[col].replace(values, [0,1], inplace=True)\n",
    "                if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        return df, self._check_binary(df, col)\n",
    "        \n",
    "        \n",
    "    def _check_type(self, df, col):\n",
    "        from pandas.api.types import is_string_dtype\n",
    "        from pandas.api.types import is_numeric_dtype\n",
    "        dtype = None\n",
    "        \n",
    "        tmp_var = df[df[col].notnull()][col]\n",
    "        #if tmp_var.nunique()<=5: return 'cat'\n",
    "        if is_numeric_dtype(tmp_var): dtype = 'numeric'\n",
    "        elif is_string_dtype(tmp_var): dtype =  'string'\n",
    "        else:\n",
    "            raise ValueError('Columns in dataframe must be either numeric or strings.  ' +\\\n",
    "                             'Column %s is neither' % (col))\n",
    "        output = {'dtype' : dtype, 'nunique' : tmp_var.nunique()}\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def _check_binary(self, df, col):\n",
    "        return df[col].isin([0,1]).all()        \n",
    "\n",
    "    def _get_feature_names(self, df):\n",
    "        return [c for c in df.columns.values \\\n",
    "                if c not in [self.treatment_col, self.outcome_col]+self.ignore_cols]\n",
    "    \n",
    "    def fit(self):\n",
    "        print(\"start fitting causal inference model\")\n",
    "        start_time = time.time()\n",
    "        self.model.fit(self.x.values, self.treatment.values, self.y.values)\n",
    "        preds = self.predict(self.x)\n",
    "        self.df[self.te] = preds\n",
    "        print(\"time to fit causal inference model: \",-start_time + time.time(),\" sec\")\n",
    "            \n",
    "    def predict(self, x):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            return self.model.predict(x.values)\n",
    "        else:\n",
    "            return self.model.predict(x)\n",
    "    \n",
    "    def estimate_ate(self, bool_mask=None):\n",
    "        df = self.df if bool_mask is None else self.df[bool_mask]\n",
    "        a = df[self.te].values\n",
    "        mean = np.mean(a)\n",
    "        return {'ate' : mean}\n",
    "        \n",
    "\n",
    "        \n",
    "    def minimize_bias(self, caliper = None):\n",
    "            print('-------Start bias minimization procedure----------')\n",
    "            start_time = time.time()\n",
    "            #Join x, y and treatment vectors\n",
    "            df_match = self.x.merge(self.treatment,left_index=True, right_index=True)\n",
    "            df_match = df_match.merge(self.y, left_index=True, right_index=True)\n",
    "\n",
    "            #buld propensity model. Propensity is the probability of raw belongs to control group.\n",
    "            pm = ElasticNetPropensityModel(n_fold=3, random_state=42)\n",
    "\n",
    "            #ps - propensity score\n",
    "            df_match['ps'] = pm.fit_predict(self.x, self.treatment)\n",
    "\n",
    "            #Matching model object\n",
    "            psm = NearestNeighborMatch(replace=False,\n",
    "                           ratio=1,\n",
    "                           random_state=423,\n",
    "                           caliper=caliper)\n",
    "\n",
    "            ps_cols = list(self.feature_names_one_hot)\n",
    "            ps_cols.append('ps')\n",
    "\n",
    "            #Apply matching model\n",
    "            #If error, then sample is unbiased and we don't do anything\n",
    "            self.flg_bias = True\n",
    "            self.df_unbiased = psm.match(data=df_match, treatment_col='treatment',score_cols=['ps'])\n",
    "            self.x_unbiased = self.df_unbiased[self.x.columns]\n",
    "            self.y_unbiased = self.df_unbiased[self.outcome_col]\n",
    "            self.treatment_unbiased = self.df_unbiased['treatment']\n",
    "            print('-------------------MATCHING RESULTS----------------')\n",
    "            print('-----BEFORE MATCHING-------')\n",
    "            print(create_table_one(data=df_match,\n",
    "                                    treatment_col='treatment',\n",
    "                                    features=list(self.feature_names_one_hot)))\n",
    "            print('-----AFTER MATCHING-------')\n",
    "            print(create_table_one(data=self.df_unbiased,\n",
    "                                    treatment_col='treatment',\n",
    "                                    features=list(self.feature_names_one_hot)))\n",
    "            return self.df_unbiased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Do social media posts by men get shared more often than those by women?\n",
    "\n",
    "Let's create a simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Male?</th>\n",
       "      <th>Post_Text</th>\n",
       "      <th>Post_Shared?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Is_Male?              Post_Text  Post_Shared?\n",
       "0         0  I really love my job!             0\n",
       "1         0  I really love my job!             0\n",
       "2         0  I really love my job!             0\n",
       "3         0  I really love my job!             0\n",
       "4         0  I really love my job!             0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "data = ((*a, b) for (a, b) in zip(itertools.product([0,1], [0,1], [0,1]), [36, 234, 25, 55, 6, 81, 71, 192]))\n",
    "df = pd.DataFrame(data, columns=['Is_Male?', 'Post_Text', 'Post_Shared?', 'N'])\n",
    "df = df.loc[df.index.repeat(df['N'])].reset_index(drop=True).drop(columns=['N'])\n",
    "values = sorted(df['Post_Text'].unique())\n",
    "df['Post_Text'].replace(values, ['I really love my job!', 'My boss is pretty terrible.'], inplace=True)\n",
    "original_df = df.copy()\n",
    "df = None\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it seems like posts by women get shared more often.  More specifically, it appears that being male **reduces** your the chance your post is shared by 4.5 percentage points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_probability = original_df[(original_df['Is_Male?']==1)]['Post_Shared?'].value_counts(normalize=True)[1]\n",
    "male_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8257142857142857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_probability = original_df[(original_df['Is_Male?']==0)]['Post_Shared?'].value_counts(normalize=True)[1]\n",
    "female_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04571428571428571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_probability-female_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is inaccurate. This is an example of [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox), and the true causal effect of being male in this simulated datsaet is roughly **0.05** (as opposed to **-0.045**) with men's posts being more likely to be shared. The reason is that women in this simulation tend to make more positive posts which tend to be shared more often here. Post sentiment, then, is a [confounder](https://en.wikipedia.org/wiki/Confounding).   When controlling for the sentiment of the post, it is revealed that men's posts are, in fact, shared more often (for both negative posts and positive posts). This can be quickly and easily estimated in **CausalNLP**.\n",
    "\n",
    "#### Causal Inference from Text with Autocoders\n",
    "\n",
    "Let's first use the `Autocoder` to transform the raw text into sentiment.  We can then control for sentiment when estimating the causal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnlp.autocoder import Autocoder\n",
    "ac = Autocoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ac.sentiment(original_df['Post_Text'].values, original_df, binarize=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Male?</th>\n",
       "      <th>Post_Text</th>\n",
       "      <th>Post_Shared?</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I really love my job!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Is_Male?              Post_Text  Post_Shared?  negative  positive\n",
       "0         0  I really love my job!             0         0         1\n",
       "1         0  I really love my job!             0         0         1\n",
       "2         0  I really love my job!             0         0         1\n",
       "3         0  I really love my job!             0         0         1\n",
       "4         0  I really love my job!             0         0         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When autocoding the raw text for sentiment, we have chosen to binarize the probabilities (`binarize=True`).  Raw probabilities can also be used with `binarize=False`.\n",
    "\n",
    "Next, let's estimate the treatment effects. We will ignore the `positive` and `Post_Shared?` columns, as their information is captured by the `negative` column in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome is: categorical\n",
      "preprocess time:  0.012778282165527344  sec\n",
      "start fitting causal inference model\n",
      "time to fit causal inference model:  1.3688530921936035  sec\n"
     ]
    }
   ],
   "source": [
    "cm = CausalInferenceModel(df, treatment_col='Is_Male?', outcome_col='Post_Shared?', \n",
    "                          ignore_cols=['positive', 'Post_Text'])\n",
    "cm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon controlling for sentiment, we see that the overall average treatment is correctly estimated as 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.05366850622769351}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate = cm.estimate_ate()\n",
    "ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CausalNLP** allows you to easily compute conditional or individualized treatment effects.\n",
    "For instance, for negative posts, being male increases the chance of your post being shared by about 4 percentage points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.042535751074149745}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(cm.df['negative']==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive posts, being male increases the chance of your post being shared by about 6 percentage points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ate': 0.06436468274776497}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.estimate_ate(cm.df['negative']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
