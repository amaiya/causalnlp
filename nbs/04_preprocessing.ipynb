{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> Preprocesses dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import time\n",
    "\n",
    "\n",
    "class DataframePreprocessor:\n",
    "    \"\"\"\n",
    "    Preproceses a pandas DataFrame for causal inference\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 treatment_col='treatment', \n",
    "                 outcome_col='outcome', \n",
    "                 text_col=None,\n",
    "                 include_cols=[],\n",
    "                 ignore_cols=[],\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Instantiates the DataframePreprocessor instance.\n",
    "        \"\"\"\n",
    "        self.treatment_col = treatment_col\n",
    "        self.outcome_col = outcome_col\n",
    "        self.text_col = text_col\n",
    "        self.include_cols = include_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.v = verbose\n",
    "\n",
    "        \n",
    "        # these variables set by preprocess\n",
    "        self.feature_names = None\n",
    "        self.feature_names_one_hot = None\n",
    "        self.feature_types = {}\n",
    "        self.cat_dict = {}\n",
    "        self.tv = None\n",
    "        self.is_classification = None\n",
    "\n",
    "\n",
    "    def preprocess(self, df, \n",
    "                   training=False,\n",
    "                   min_df=0.05,\n",
    "                   max_df=0.5,\n",
    "                   ngram_range=(1,1),\n",
    "                   stop_words='english',\n",
    "                   na_cont_value=-1, na_cat_value='MISSING'):\n",
    "        \"\"\"\n",
    "        Preprocess a dataframe for causal inference.\n",
    "        \"\"\"\n",
    "        # checks\n",
    "        if not training and self.feature_names is None:\n",
    "            raise ValueError('Preprocessor must first be fitted by calling with training=True.')\n",
    "        if not isinstance(self.ignore_cols, list):\n",
    "            raise ValueError('ignore_cols must be a list.')\n",
    "        if not isinstance(self.include_cols, list):\n",
    "            raise ValueError('include_cols must be a list.')\n",
    "        if training and self.ignore_cols and self.include_cols:\n",
    "            raise  ValueError('ignore_cols and include_cols are mutually exclusive.  Please choose one.')\n",
    "        if training and self.include_cols:\n",
    "            self.ignore_cols = [c for c in df.columns.values if c not in self.include_cols +\\\n",
    "                                                                              [self.treatment_col, \n",
    "                                                                               self.outcome_col, \n",
    "                                                                               self.text_col]]\n",
    "        if self.text_col is not None and self.text_col not in df:\n",
    "            raise ValueError(f'You specified text_col=\"{self.text_col}\", but {self.text_col} is not a column in df.')\n",
    "        if self.treatment_col in self.ignore_cols:\n",
    "            raise ValueError(f'ignore_cols contains the treatment column ({self.treatment_col})')\n",
    "        if self.outcome_col in self.ignore_cols:\n",
    "            raise ValueError(f'ignore_cols contains the outcome column ({self.outcome_col})')\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # step 1: check/clean dataframe\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError('df must be a pandas DataFrame')\n",
    "        df = df.rename(columns=lambda x: x.strip()) # strip headers \n",
    "        # check and re-order test DataFrame\n",
    "        if not training:\n",
    "            test_feats = [col.strip() for col in df.columns.values if col.strip() in self.feature_names]\n",
    "            if len( set(test_feats) & set(self.feature_names) ) != len(self.feature_names):\n",
    "                raise ValueError('df must contain the same columns as DataFrame used for training model.')\n",
    "            if self.treatment_col not in df.columns:\n",
    "                raise ValueError(f'Column {self.treatment_col} is missing from df.')\n",
    "            if self.text_col is not None and self.text_col not in df.columns.values:\n",
    "                raise ValueError(f'Colummn {self.text_col} is missing from df')               \n",
    "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)  # strip data\n",
    "        df, _ = self._preprocess_column(df, self.treatment_col, is_treatment=True)\n",
    "        if training:\n",
    "            df, self.is_classification = self._preprocess_column(df, \n",
    "                                                                 self.outcome_col, is_treatment=False)\n",
    "            self.feature_names = [c for c in df.columns.values \\\n",
    "                                  if c not in [self.treatment_col, \n",
    "                                              self.outcome_col, self.text_col]+self.ignore_cols]\n",
    "            for c in self.feature_names:\n",
    "                self.feature_types[c] = self._check_type(df, c)['dtype']\n",
    "        X = df[self.feature_names].copy()\n",
    "        Y = df[self.outcome_col].copy() if training else None\n",
    "        T = df[self.treatment_col].copy()   \n",
    "\n",
    "        # step 2: fill empty values on x\n",
    "        for c in self.feature_names:\n",
    "            dtype = self.feature_types[c]  \n",
    "            if dtype == 'string': X[c] = X[c].fillna(na_cat_value)\n",
    "            if dtype == 'numeric': X[c] = X[c].fillna(na_cont_value)\n",
    "                        \n",
    "        # step 3: one-hot encode categorial features\n",
    "        for c in self.feature_names:\n",
    "            if c == self.text_col: continue\n",
    "            if self.feature_types[c] == 'string':\n",
    "                if df.shape[0] > 100 and df[c].nunique()/df.shape[0] > 0.5:\n",
    "                    if self.text_col is not None:\n",
    "                        err_msg = f'Column \"{c}\" looks like it contains free-form text. ' +\\\n",
    "                        f'Since there is already a text_col specified ({self.text_col}), '+\\\n",
    "                        f'you should probably include this column in the \"ignore_cols\" list.'\n",
    "                    else:\n",
    "                        err_msg = f'Column \"{c}\" looks like it contains free-form text or ' +\\\n",
    "                        f'or unique values. Please either set text_col=\"{c}\" or add it to \"ignore_cols\" list.'\n",
    "                    raise ValueError(err_msg)\n",
    "                      \n",
    "                if training:\n",
    "                    self.cat_dict[c] = sorted(X[c].unique())\n",
    "                    catcol = X[c]\n",
    "                else:\n",
    "                    #REF: https://stackoverflow.com/a/37451867/13550699\n",
    "                    catcol = X[c].astype(pd.CategoricalDtype(categories=self.cat_dict[c]))\n",
    "                X = X.merge(pd.get_dummies(catcol, prefix = c, \n",
    "                                                     drop_first=False), \n",
    "                                                     left_index=True, right_index=True)\n",
    "                \n",
    "                del X[c]\n",
    "        self.feature_names_one_hot = X.columns\n",
    "        \n",
    "                        \n",
    "        # step 4: for text-based confounder, use extracted vocabulary as features\n",
    "        if self.text_col is not None:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            if training:\n",
    "                self.tv = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                         ngram_range=ngram_range, stop_words=stop_words)\n",
    "                v_features = self.tv.fit_transform(df[self.text_col])\n",
    "            else:\n",
    "                v_features = self.tv.transform(df[self.text_col])\n",
    "            vocab = self.tv.get_feature_names()\n",
    "            vocab_df = pd.DataFrame(v_features.toarray(), columns = [\"v_%s\" % (v) for v in vocab])\n",
    "            X = pd.concat([X, vocab_df], axis=1, join='inner')\n",
    "        outcome_type = 'categorical' if self.is_classification else 'numerical'\n",
    "        if training:\n",
    "            if self.outcome_col in df.columns and self.v:\n",
    "                print(f'outcome column ({outcome_type}): {self.outcome_col}')\n",
    "            if self.v: print(f'treatment column: {self.treatment_col}')\n",
    "            if self.v: print('numerical/categorical covariates: %s' % (self.feature_names))\n",
    "            if self.v and self.text_col: print('text covariate: %s' % (self.text_col))\n",
    "            if self.v: print(\"preprocess time: \", -start_time + time.time(),\" sec\")\n",
    "        return (df, X, Y, T)\n",
    "        \n",
    "        \n",
    "    def _preprocess_column(self, df, col, is_treatment=True):\n",
    "        \"\"\"\n",
    "        Preprocess treatment and outcome columns.\n",
    "        \"\"\"\n",
    "        # remove nulls\n",
    "        df = df[df[col].notnull()]\n",
    "\n",
    "        # check if already binarized\n",
    "        if self._check_binary(df, col): return df, True\n",
    "\n",
    "        # inspect column\n",
    "        d = self._check_type(df, col)\n",
    "        typ = d['dtype']\n",
    "        num = d['nunique']\n",
    "        \n",
    "        # process as treatment\n",
    "        if is_treatment:\n",
    "            if typ == 'numeric' or (typ == 'string' and num != 2): \n",
    "                raise ValueError('Treatment column must contain only two unique values ' +\\\n",
    "                                 'indicating the treated and control groups.')\n",
    "            values = sorted(df[col].unique())\n",
    "            df[col].replace(values, [0,1], inplace=True)\n",
    "            if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        # process as outcome\n",
    "        else:\n",
    "            if typ == 'string' and num != 2:\n",
    "                raise ValueError('If the outcome column is string/categorical, it must '+\n",
    "                                'contain only two unique values.')\n",
    "            if typ == 'string':\n",
    "                values = sorted(df[col].unique())\n",
    "                df[col].replace(values, [0,1], inplace=True)\n",
    "                if self.v: print('replaced %s in column \"%s\" with %s' % (values, col, [0,1]))\n",
    "        return df, self._check_binary(df, col)\n",
    "        \n",
    "        \n",
    "    def _check_type(self, df, col):\n",
    "        from pandas.api.types import is_string_dtype\n",
    "        from pandas.api.types import is_numeric_dtype\n",
    "        dtype = None\n",
    "        \n",
    "        tmp_var = df[df[col].notnull()][col]\n",
    "        if is_numeric_dtype(tmp_var): dtype = 'numeric'\n",
    "        elif is_string_dtype(tmp_var): dtype =  'string'\n",
    "        else:\n",
    "            raise ValueError('Columns in dataframe must be either numeric or strings.  ' +\\\n",
    "                             'Column %s is neither' % (col))\n",
    "        output = {'dtype' : dtype, 'nunique' : tmp_var.nunique()}\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def _check_binary(self, df, col):\n",
    "        return df[col].isin([0,1]).all()        \n",
    "\n",
    "    def _get_feature_names(self, df):\n",
    "        return [c for c in df.columns.values \\\n",
    "                if c not in [self.treatment_col, self.outcome_col]+self.ignore_cols]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DataframePreprocessor.preprocess\" class=\"doc_header\"><code>DataframePreprocessor.preprocess</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DataframePreprocessor.preprocess</code>(**`df`**, **`training`**=*`False`*, **`min_df`**=*`0.05`*, **`max_df`**=*`0.5`*, **`ngram_range`**=*`(1, 1)`*, **`stop_words`**=*`'english'`*, **`na_cont_value`**=*`-1`*, **`na_cat_value`**=*`'MISSING'`*)\n",
       "\n",
       "Preprocess a dataframe for causal inference."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DataframePreprocessor.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sample_data/music_seed50.tsv', sep='\\t', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = DataframePreprocessor(treatment_col='T_ac', outcome_col='Y_sim', \n",
    "                           text_col='text', include_cols=['C_true', 'product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome column (categorical): Y_sim\n",
      "treatment column: T_ac\n",
      "numerical/categorical covariates: ['product', 'C_true']\n",
      "text covariate: text\n",
      "preprocess time:  1.49556303024292  sec\n"
     ]
    }
   ],
   "source": [
    "df, X, Y, T = pp.preprocess(df, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_true</th>\n",
       "      <th>product_audio cd</th>\n",
       "      <th>product_mp3 music</th>\n",
       "      <th>product_vinyl</th>\n",
       "      <th>v_album</th>\n",
       "      <th>v_albums</th>\n",
       "      <th>v_band</th>\n",
       "      <th>v_beautiful</th>\n",
       "      <th>v_best</th>\n",
       "      <th>v_better</th>\n",
       "      <th>v_bought</th>\n",
       "      <th>v_buy</th>\n",
       "      <th>v_cd</th>\n",
       "      <th>v_collection</th>\n",
       "      <th>v_did</th>\n",
       "      <th>v_don</th>\n",
       "      <th>v_excellent</th>\n",
       "      <th>v_fan</th>\n",
       "      <th>v_favorite</th>\n",
       "      <th>v_good</th>\n",
       "      <th>v_got</th>\n",
       "      <th>v_great</th>\n",
       "      <th>v_hear</th>\n",
       "      <th>v_heard</th>\n",
       "      <th>v_just</th>\n",
       "      <th>v_know</th>\n",
       "      <th>v_like</th>\n",
       "      <th>v_listen</th>\n",
       "      <th>v_listening</th>\n",
       "      <th>v_love</th>\n",
       "      <th>v_music</th>\n",
       "      <th>v_new</th>\n",
       "      <th>v_old</th>\n",
       "      <th>v_original</th>\n",
       "      <th>v_really</th>\n",
       "      <th>v_record</th>\n",
       "      <th>v_recording</th>\n",
       "      <th>v_rock</th>\n",
       "      <th>v_song</th>\n",
       "      <th>v_songs</th>\n",
       "      <th>v_sound</th>\n",
       "      <th>v_sounds</th>\n",
       "      <th>v_think</th>\n",
       "      <th>v_time</th>\n",
       "      <th>v_track</th>\n",
       "      <th>v_tracks</th>\n",
       "      <th>v_ve</th>\n",
       "      <th>v_voice</th>\n",
       "      <th>v_way</th>\n",
       "      <th>v_work</th>\n",
       "      <th>v_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850798</td>\n",
       "      <td>0.251679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625138</td>\n",
       "      <td>0.561398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.629106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372982</td>\n",
       "      <td>0.334952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_true  product_audio cd  product_mp3 music  product_vinyl  v_album  \\\n",
       "0       0                 0                  1              0  0.25232   \n",
       "1       0                 0                  1              0  0.00000   \n",
       "2       1                 1                  0              0  0.00000   \n",
       "3       0                 0                  1              0  0.00000   \n",
       "4       1                 1                  0              0  0.00000   \n",
       "\n",
       "   v_albums  v_band  v_beautiful  v_best  v_better  v_bought     v_buy  \\\n",
       "0       0.0     0.0          0.0     0.0       0.0       0.0  0.850798   \n",
       "1       0.0     0.0          0.0     0.0       0.0       0.0  0.000000   \n",
       "2       0.0     0.0          0.0     0.0       0.0       0.0  0.000000   \n",
       "3       0.0     0.0          0.0     0.0       0.0       0.0  0.000000   \n",
       "4       0.0     0.0          0.0     0.0       0.0       0.0  0.000000   \n",
       "\n",
       "       v_cd  v_collection  v_did     v_don  v_excellent  v_fan  v_favorite  \\\n",
       "0  0.251679           0.0    0.0  0.386181          0.0    0.0         0.0   \n",
       "1  0.000000           0.0    0.0  0.000000          0.0    0.0         0.0   \n",
       "2  0.542250           0.0    0.0  0.000000          0.0    0.0         0.0   \n",
       "3  0.000000           0.0    0.0  0.000000          0.0    0.0         0.0   \n",
       "4  0.000000           0.0    0.0  0.000000          0.0    0.0         0.0   \n",
       "\n",
       "   v_good  v_got  v_great  v_hear   v_heard  v_just    v_know    v_like  \\\n",
       "0     0.0    0.0      0.0     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "1     0.0    0.0      0.0     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "2     0.0    0.0      0.0     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "3     0.0    0.0      0.0     0.0  0.000000     0.0  0.629106  0.000000   \n",
       "4     0.0    0.0      0.0     0.0  0.527751     0.0  0.000000  0.392572   \n",
       "\n",
       "   v_listen  v_listening    v_love   v_music  v_new  v_old  v_original  \\\n",
       "0       0.0          0.0  0.000000  0.000000    0.0    0.0     0.00000   \n",
       "1       0.0          0.0  0.000000  0.000000    0.0    0.0     0.00000   \n",
       "2       0.0          0.0  0.625138  0.561398    0.0    0.0     0.00000   \n",
       "3       0.0          0.0  0.000000  0.777319    0.0    0.0     0.00000   \n",
       "4       0.0          0.0  0.372982  0.334952    0.0    0.0     0.56219   \n",
       "\n",
       "   v_really  v_record  v_recording  v_rock  v_song  v_songs  v_sound  \\\n",
       "0       0.0       0.0          0.0     0.0     0.0      0.0      0.0   \n",
       "1       0.0       0.0          0.0     0.0     0.0      0.0      0.0   \n",
       "2       0.0       0.0          0.0     0.0     0.0      0.0      0.0   \n",
       "3       0.0       0.0          0.0     0.0     0.0      0.0      0.0   \n",
       "4       0.0       0.0          0.0     0.0     0.0      0.0      0.0   \n",
       "\n",
       "   v_sounds  v_think  v_time  v_track  v_tracks  v_ve  v_voice  v_way  v_work  \\\n",
       "0       0.0      0.0     0.0      0.0       0.0   0.0      0.0    0.0     0.0   \n",
       "1       0.0      0.0     0.0      0.0       0.0   0.0      0.0    0.0     0.0   \n",
       "2       0.0      0.0     0.0      0.0       0.0   0.0      0.0    0.0     0.0   \n",
       "3       0.0      0.0     0.0      0.0       0.0   0.0      0.0    0.0     0.0   \n",
       "4       0.0      0.0     0.0      0.0       0.0   0.0      0.0    0.0     0.0   \n",
       "\n",
       "   v_years  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_true</th>\n",
       "      <th>product</th>\n",
       "      <th>text</th>\n",
       "      <th>Y_sim</th>\n",
       "      <th>T_ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>vinyl</td>\n",
       "      <td>This record hurts my ears.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mp3 music</td>\n",
       "      <td>The music of Yanni is beautiful and breath-tak...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_true    product                                               text  \\\n",
       "0       0      vinyl                         This record hurts my ears.   \n",
       "1       1  mp3 music  The music of Yanni is beautiful and breath-tak...   \n",
       "\n",
       "   Y_sim  T_ac  \n",
       "0      0     0  \n",
       "1      1     1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'C_true' : [0, 1],\n",
    "    'product': ['vinyl', 'mp3 music'],\n",
    "     'text' : ['This record hurts my ears.', \"The music of Yanni is beautiful and breath-taking.\"],\n",
    "    'Y_sim' : [0, 1],\n",
    "     'T_ac' : [0, 1],\n",
    "      })\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_test, _, _ = pp.preprocess(test_df, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([X_test.columns.values[i] == col for i,col in enumerate(X.columns.values)]) == len(X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'product': ['vinyl', 'mp3 music'],\n",
    "     'text' : ['This record hurts my ears.', \"The music of Yanni is beautiful and breath-taking.\"],\n",
    "    'Y_sim' : [0, 1],\n",
    "     'T_ac' : [0, 1],\n",
    "      })\n",
    "error = False\n",
    "try: \n",
    "    _, X_test, _, _ = pp.preprocess(test_df, training=False)\n",
    "except ValueError:\n",
    "    error = True\n",
    "assert error is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted 02_analyzers.ipynb.\n",
      "Converted 03_preprocessing.ipynb.\n",
      "Converted 99_examples.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
