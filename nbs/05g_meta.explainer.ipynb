{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Metalearner Expainer\n",
    "output-file: meta.explainer.html\n",
    "title: Metalearner Explainer\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp meta.explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# REFERENCE: https://github.com/uber/causalml\n",
    "\n",
    "# Copyright 2019 Uber Technology, Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "\n",
    "from causalnlp.meta.utils import convert_pd_to_np\n",
    "\n",
    "VALID_METHODS = ('auto', 'permutation', 'shapley')\n",
    "\n",
    "\n",
    "class Explainer(object):\n",
    "    def __init__(self, method, control_name, X, tau, classes, model_tau=None,\n",
    "                 features=None, normalize=True, test_size=0.3, random_state=None, override_checks=False,\n",
    "                 r_learners=None):\n",
    "        \"\"\"\n",
    "        The Explainer class handles all feature explanation/interpretation functions, including plotting\n",
    "        feature importances, shapley value distributions, and shapley value dependency plots.\n",
    "\n",
    "        Currently supported methods are:\n",
    "            - auto (calculates importance based on estimator's default implementation of feature importance;\n",
    "                    estimator must be tree-based)\n",
    "                    Note: if none provided, it uses lightgbm's LGBMRegressor as estimator, and \"gain\" as\n",
    "                    importance type\n",
    "            - permutation (calculates importance based on mean decrease in accuracy when a feature column is permuted;\n",
    "                           estimator can be any form)\n",
    "            - shapley (calculates shapley values; estimator must be tree-based)\n",
    "        Hint: for permutation, downsample data for better performance especially if X.shape[1] is large\n",
    "\n",
    "        Args:\n",
    "            method (str): auto, permutation, shapley\n",
    "            control_name (str/int/float): name of control group\n",
    "            X (np.matrix): a feature matrix\n",
    "            tau (np.array): a treatment effect vector (estimated/actual)\n",
    "            classes (dict): a mapping of treatment names to indices (used for indexing tau array)\n",
    "            model_tau (sklearn/lightgbm/xgboost model object): a model object\n",
    "            features (np.array): list/array of feature names. If None, an enumerated list will be used.\n",
    "            normalize (bool): normalize by sum of importances if method=auto (defaults to True)\n",
    "            test_size (float/int): if float, represents the proportion of the dataset to include in the test split.\n",
    "                                   If int, represents the absolute number of test samples (used for estimating\n",
    "                                   permutation importance)\n",
    "            random_state (int/RandomState instance/None): random state used in permutation importance estimation\n",
    "            override_checks (bool): overrides self.check_conditions (e.g. if importance/shapley values are pre-computed)\n",
    "            r_learners (dict): a mapping of treatment group to fitted R Learners\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.control_name = control_name\n",
    "        self.X = convert_pd_to_np(X)\n",
    "        self.tau = convert_pd_to_np(tau)\n",
    "        if self.tau is not None and self.tau.ndim == 1:\n",
    "            self.tau = self.tau.reshape(-1, 1)\n",
    "        self.classes = classes\n",
    "        self.model_tau = LGBMRegressor(importance_type='gain') if model_tau is None else model_tau\n",
    "        self.features = features\n",
    "        self.normalize = normalize\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.override_checks = override_checks\n",
    "        self.r_learners = r_learners\n",
    "\n",
    "        if not self.override_checks:\n",
    "            self.check_conditions()\n",
    "            self.create_feature_names()\n",
    "            self.build_new_tau_models()\n",
    "\n",
    "    def check_conditions(self):\n",
    "        \"\"\"\n",
    "        Checks for multiple conditions:\n",
    "            - method is valid\n",
    "            - X, tau, and classes are specified\n",
    "            - model_tau has feature_importances_ attribute after fitting\n",
    "        \"\"\"\n",
    "        assert self.method in VALID_METHODS, 'Current supported methods: {}'.format(', '.join(VALID_METHODS))\n",
    "\n",
    "        assert all(obj is not None for obj in (self.X, self.tau, self.classes)), \\\n",
    "            \"X, tau, and classes must be provided.\"\n",
    "\n",
    "        model_test = deepcopy(self.model_tau)\n",
    "        model_test.fit([[0], [1]], [0, 1])  # Fit w/ dummy data to check for feature_importances_ below\n",
    "        assert hasattr(model_test, \"feature_importances_\"), \\\n",
    "            \"model_tau must have the feature_importances_ method (after fitting)\"\n",
    "\n",
    "    def create_feature_names(self):\n",
    "        \"\"\"\n",
    "        Creates feature names (simple enumerated list) if not provided in __init__.\n",
    "        \"\"\"\n",
    "        if self.features is None:\n",
    "            num_features = self.X.shape[1]\n",
    "            self.features = ['Feature_{:03d}'.format(i) for i in range(num_features)]\n",
    "\n",
    "    def build_new_tau_models(self):\n",
    "        \"\"\"\n",
    "        Builds tau models (using X to predict estimated/actual tau) for each treatment group.\n",
    "        \"\"\"\n",
    "        if self.method in ('permutation'):\n",
    "            self.X_train, self.X_test, self.tau_train, self.tau_test = train_test_split(self.X,\n",
    "                                                                                        self.tau,\n",
    "                                                                                        test_size=self.test_size,\n",
    "                                                                                        random_state=self.random_state)\n",
    "        else:\n",
    "            self.X_train, self.tau_train = self.X, self.tau\n",
    "\n",
    "        if self.r_learners is not None:\n",
    "            self.models_tau = deepcopy(self.r_learners)\n",
    "        else:\n",
    "            self.models_tau = {group: deepcopy(self.model_tau) for group in self.classes}\n",
    "            for group, idx in self.classes.items():\n",
    "                self.models_tau[group].fit(self.X_train, self.tau_train[:, idx])\n",
    "\n",
    "    def get_importance(self):\n",
    "        \"\"\"\n",
    "        Calculates feature importances for each treatment group, based on specified method in __init__.\n",
    "        \"\"\"\n",
    "        importance_catalog = {'auto': self.default_importance, 'permutation': self.perm_importance}\n",
    "        importance_dict = importance_catalog[self.method]()\n",
    "\n",
    "        importance_dict = {group: pd.Series(array, index=self.features).sort_values(ascending=False)\n",
    "                           for group, array in importance_dict.items()}\n",
    "        return importance_dict\n",
    "\n",
    "    def default_importance(self):\n",
    "        \"\"\"\n",
    "        Calculates feature importances for each treatment group, based on the model_tau's default implementation.\n",
    "        \"\"\"\n",
    "        importance_dict = {}\n",
    "        if self.r_learners is not None:\n",
    "            self.models_tau = deepcopy(self.r_learners)\n",
    "        for group, idx in self.classes.items():\n",
    "            importance_dict[group] = self.models_tau[group].feature_importances_\n",
    "            if self.normalize:\n",
    "                importance_dict[group] = importance_dict[group] / importance_dict[group].sum()\n",
    "\n",
    "        return importance_dict\n",
    "\n",
    "    def perm_importance(self):\n",
    "        \"\"\"\n",
    "        Calculates feature importances for each treatment group, based on the permutation method.\n",
    "        \"\"\"\n",
    "        importance_dict = {}\n",
    "        if self.r_learners is not None:\n",
    "            self.models_tau = deepcopy(self.r_learners)\n",
    "            self.X_test, self.tau_test = self.X, self.tau\n",
    "        for group, idx in self.classes.items():\n",
    "            perm_estimator = self.models_tau[group]\n",
    "            importance_dict[group] = permutation_importance(estimator=perm_estimator,\n",
    "                                                            X=self.X_test,\n",
    "                                                            y=self.tau_test[:, idx],\n",
    "                                                            random_state=self.random_state).importances_mean\n",
    "\n",
    "        return importance_dict\n",
    "\n",
    "    def get_shap_values(self):\n",
    "        \"\"\"\n",
    "        Calculates shapley values for each treatment group.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import shap\n",
    "        except ImportError:\n",
    "            raise ImportError('Please install shap (conda is recommended): '+\\\n",
    "                              'conda install shap --channel conda-forge')\n",
    "            \n",
    "        shap_dict = {}\n",
    "        for group, mod in self.models_tau.items():\n",
    "            explainer = shap.TreeExplainer(mod)\n",
    "            if self.r_learners is not None:\n",
    "                explainer.model.original_model.params['objective'] = None  # hacky way of running shap without error\n",
    "            shap_values = explainer.shap_values(self.X)\n",
    "            shap_dict[group] = shap_values\n",
    "\n",
    "        return shap_dict\n",
    "\n",
    "    def plot_importance(self, importance_dict=None, title_prefix=''):\n",
    "        \"\"\"\n",
    "        Calculates and plots feature importances for each treatment group, based on specified method in __init__.\n",
    "        Skips the calculation part if importance_dict is given.\n",
    "        \"\"\"\n",
    "        if importance_dict is None:\n",
    "            importance_dict = self.get_importance()\n",
    "        for group, series in importance_dict.items():\n",
    "            plt.figure()\n",
    "            series.sort_values().plot(kind='barh', figsize=(12, 8))\n",
    "            title = group\n",
    "            if title_prefix != '':\n",
    "                title = '{} - {}'.format(title_prefix, title)\n",
    "            plt.title(title)\n",
    "\n",
    "    def plot_shap_values(self, shap_dict=None):\n",
    "        \"\"\"\n",
    "        Calculates and plots the distribution of shapley values of each feature, for each treatment group.\n",
    "        Skips the calculation part if shap_dict is given.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import shap\n",
    "        except ImportError:\n",
    "            raise ImportError('Please install shap (conda is recommended): '+\\\n",
    "                              'conda install shap --channel conda-forge')\n",
    "            \n",
    "        if shap_dict is None:\n",
    "            shap_dict = self.get_shap_values()\n",
    "\n",
    "        for group, values in shap_dict.items():\n",
    "            plt.title(group)\n",
    "            shap.summary_plot(values, features=self.X, feature_names=self.features)\n",
    "\n",
    "    def plot_shap_dependence(self, treatment_group, feature_idx, shap_dict=None, interaction_idx='auto', **kwargs):\n",
    "        \"\"\"\n",
    "        Plots dependency of shapley values for a specified feature, colored by an interaction feature.\n",
    "        Skips the calculation part if shap_dict is given.\n",
    "\n",
    "        This plots the value of the feature on the x-axis and the SHAP value of the same feature\n",
    "        on the y-axis. This shows how the model depends on the given feature, and is like a\n",
    "        richer extension of the classical partial dependence plots. Vertical dispersion of the\n",
    "        data points represents interaction effects.\n",
    "\n",
    "       Args:\n",
    "            treatment_group (str or int): name of treatment group to create dependency plot on\n",
    "            feature_idx (str or int): feature index / name to create dependency plot on\n",
    "            shap_dict (optional, dict): a dict of shapley value matrices. If None, shap_dict will be computed.\n",
    "            interaction_idx (optional, str or int): feature index / name used in coloring scheme as interaction feature.\n",
    "                If \"auto\" then shap.common.approximate_interactions is used to pick what seems to be the\n",
    "                strongest interaction (note that to find to true strongest interaction you need to compute\n",
    "                the SHAP interaction values).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import shap\n",
    "        except ImportError:\n",
    "            raise ImportError('Please install shap (conda is recommended): '+\\\n",
    "                              'conda install shap --channel conda-forge')\n",
    "        if shap_dict is None:\n",
    "            shap_dict = self.get_shap_values()\n",
    "\n",
    "        shap_values = shap_dict[treatment_group]\n",
    "\n",
    "        shap.dependence_plot(feature_idx, shap_values, self.X, interaction_index=interaction_idx,\n",
    "                             feature_names=self.features, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
