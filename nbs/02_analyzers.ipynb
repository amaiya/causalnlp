{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14123b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f5a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537864b",
   "metadata": {},
   "source": [
    "# Analyzers\n",
    "\n",
    "> Text analyzers to help create text-based covariates, treatments, or outcomes for causal analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4e7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca57df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def list2chunks(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84543c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ZeroShotClassifier():\n",
    "    \"\"\"\n",
    "    Interface to Zero Shot Topic Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='facebook/bart-large-mnli', device=None):\n",
    "        \"\"\"\n",
    "        ZeroShotClassifier constructor\n",
    "\n",
    "        Args:\n",
    "          model_name(str): name of a BART NLI model\n",
    "          device(str): device to use (e.g., 'cuda', 'cpu')\n",
    "        \"\"\"\n",
    "        if 'mnli' not in model_name and 'xnli' not in model_name:\n",
    "            raise ValueError('ZeroShotClasifier requires an MNLI or XNLI model')\n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            raise Exception('ZeroShotClassifier requires PyTorch to be installed.')\n",
    "        self.torch_device = device\n",
    "        if self.torch_device is None: self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.torch_device)\n",
    "\n",
    "\n",
    "    def predict(self, docs, labels=[], include_labels=False, multilabel=True,\n",
    "               max_length=512, batch_size=8, nli_template='This text is about {}.',  topic_strings=[]):\n",
    "        \"\"\"\n",
    "        This method performs zero-shot text classification using Natural Language Inference (NLI).\n",
    "\n",
    "\n",
    "        **Parameters**:\n",
    "          - docs(list|str): text of document or list of texts\n",
    "          - labels(list): a list of strings representing topics of your choice\n",
    "                          Example:\n",
    "                           labels=['political science', 'sports', 'science']\n",
    "          - include_labels(bool): If True, will return topic labels along with topic probabilities\n",
    "          - multilabel(bool): If True, labels are considered independent and multiple labels can predicted true for document and be close to 1.\n",
    "                            If False, scores are normalized such that probabilities sum to 1.\n",
    "          - max_length(int): truncate long documents to this many tokens\n",
    "          - batch_size(int): batch_size to use. default:8\n",
    "                           Increase this value to speed up predictions - especially\n",
    "                           if len(topic_strings) is large.\n",
    "          - nli_template(str): labels are inserted into this template for use as hypotheses in natural language inference\n",
    "          - topic_strings(list): alias for labels parameter for backwards compatibility\n",
    "          \n",
    "        **Returns:**\n",
    "        \n",
    "        \n",
    "          inferred probabilities or list of inferred probabilities if doc is list\n",
    "        \"\"\"\n",
    "\n",
    "        # error checks\n",
    "        is_str_input = False\n",
    "        if not isinstance(docs, (list, np.ndarray)): \n",
    "            docs = [docs]\n",
    "            is_str_input = True\n",
    "        if not isinstance(docs[0], str): raise ValueError('docs must be string or a list of strings representing document(s)')\n",
    "        if len(labels) > 0 and len(topic_strings) > 0: raise ValueError('labels and topic_strings are mutually exclusive')\n",
    "        if not labels and not topic_strings: raise ValueError('labels must be a list of strings')\n",
    "        if topic_strings: \n",
    "            labels = topic_strings\n",
    "\n",
    "\n",
    "        # convert to sequences\n",
    "        sequence_pairs = []\n",
    "        for premise in docs:\n",
    "            sequence_pairs.extend([[premise, nli_template.format(label)] for label in labels])\n",
    "        if batch_size  > len(sequence_pairs): batch_size = len(sequence_pairs)\n",
    "        if len(sequence_pairs) >= 100 and batch_size==8:\n",
    "            warnings.warn('TIP: Try increasing batch_size to speedup ZeroShotClassifier predictions')\n",
    "        num_chunks = math.ceil(len(sequence_pairs)/batch_size)\n",
    "        sequence_chunks = list2chunks(sequence_pairs, n=num_chunks)\n",
    "\n",
    "        # inference\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            outputs = []\n",
    "            for sequences in sequence_chunks:\n",
    "                batch = self.tokenizer.batch_encode_plus(sequences, return_tensors='pt', max_length=max_length, truncation='only_first', padding=True).to(self.torch_device)\n",
    "                logits = self.model(batch['input_ids'], attention_mask=batch['attention_mask'], return_dict=False)[0]\n",
    "                outputs.extend(logits.cpu().detach().numpy())\n",
    "        outputs = np.array(outputs)\n",
    "        outputs = outputs.reshape((len(docs), len(labels), -1))\n",
    "\n",
    "        # process outputs\n",
    "        if multilabel:\n",
    "            # softmax over the entailment vs. contradiction dim for each label independently\n",
    "            entail_contr_logits = outputs[..., [0, -1]]\n",
    "            scores = np.exp(entail_contr_logits) / np.exp(entail_contr_logits).sum(-1, keepdims=True)\n",
    "            scores = scores[..., 1]\n",
    "        else:\n",
    "            # softmax the \"entailment\" logits over all candidate labels\n",
    "            entail_logits = outputs[..., -1]\n",
    "            scores = np.exp(entail_logits) / np.exp(entail_logits).sum(-1, keepdims=True)\n",
    "        scores = scores.tolist()\n",
    "        if include_labels:\n",
    "            scores = [list(zip(labels, s)) for s in scores]\n",
    "        if is_str_input: scores = scores[0]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb21694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ZeroShotClassifier.predict\" class=\"doc_header\"><code>ZeroShotClassifier.predict</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ZeroShotClassifier.predict</code>(**`docs`**, **`labels`**=*`[]`*, **`include_labels`**=*`False`*, **`multilabel`**=*`True`*, **`max_length`**=*`512`*, **`batch_size`**=*`8`*, **`nli_template`**=*`'This text is about {}.'`*, **`topic_strings`**=*`[]`*)\n",
       "\n",
       "This method performs zero-shot text classification using Natural Language Inference (NLI).\n",
       "\n",
       "\n",
       "**Parameters**:\n",
       "  - docs(list|str): text of document or list of texts\n",
       "  - labels(list): a list of strings representing topics of your choice\n",
       "                  Example:\n",
       "                   labels=['political science', 'sports', 'science']\n",
       "  - include_labels(bool): If True, will return topic labels along with topic probabilities\n",
       "  - multilabel(bool): If True, labels are considered independent and multiple labels can predicted true for document and be close to 1.\n",
       "                    If False, scores are normalized such that probabilities sum to 1.\n",
       "  - max_length(int): truncate long documents to this many tokens\n",
       "  - batch_size(int): batch_size to use. default:8\n",
       "                   Increase this value to speed up predictions - especially\n",
       "                   if len(topic_strings) is large.\n",
       "  - nli_template(str): labels are inserted into this template for use as hypotheses in natural language inference\n",
       "  - topic_strings(list): alias for labels parameter for backwards compatibility\n",
       "  \n",
       "**Returns:**\n",
       "\n",
       "\n",
       "  inferred probabilities or list of inferred probabilities if doc is list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ZeroShotClassifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05148cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl = ZeroShotClassifier()\n",
    "labels=['politics', 'elections', 'sports', 'films', 'television']\n",
    "doc = 'I am extremely dissatisfied with the President and will definitely vote in 2020.'\n",
    "preds = zsl.predict(doc, labels=labels, include_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea2e002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('politics', 0.979189932346344),\n",
       " ('elections', 0.9874580502510071),\n",
       " ('sports', 0.0005765454261563718),\n",
       " ('films', 0.002292441902682185),\n",
       " ('television', 0.001054605352692306)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e98953",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(preds)\n",
    "assert d['politics'] > 0.9\n",
    "assert d['elections'] > 0.9\n",
    "assert d['sports'] < 0.1\n",
    "assert d['films'] < 0.1\n",
    "assert d['television'] < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565f7ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted 02_analyzers.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fefe24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
