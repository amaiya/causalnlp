{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14123b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#all_notest\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537864b",
   "metadata": {},
   "source": [
    "# Analyzers\n",
    "\n",
    "> Text analyzers to help create text-based covariates, treatments, or outcomes for causal analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def list2chunks(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84543c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ZeroShotClassifier():\n",
    "    \"\"\"\n",
    "    Interface to Zero Shot Topic Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='facebook/bart-large-mnli', device=None):\n",
    "        \"\"\"\n",
    "        ZeroShotClassifier constructor\n",
    "\n",
    "        **Args:**\n",
    "          - model_name(str): name of a BART NLI model\n",
    "          - device(str): device to use (e.g., 'cuda', 'cpu')\n",
    "        \"\"\"\n",
    "        if 'mnli' not in model_name and 'xnli' not in model_name:\n",
    "            raise ValueError('ZeroShotClasifier requires an MNLI or XNLI model')\n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            raise Exception('ZeroShotClassifier requires PyTorch to be installed.')\n",
    "        self.torch_device = device\n",
    "        if self.torch_device is None: self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.torch_device)\n",
    "\n",
    "\n",
    "    def predict(self, docs, labels=[], include_labels=False, multilabel=True,\n",
    "               max_length=512, batch_size=8, nli_template='This text is about {}.',  topic_strings=[]):\n",
    "        \"\"\"\n",
    "        This method performs zero-shot text classification using Natural Language Inference (NLI).\n",
    "\n",
    "\n",
    "        **Parameters**:\n",
    "          - docs(list|str): text of document or list of texts\n",
    "          - labels(list): a list of strings representing topics of your choice\n",
    "                          Example:\n",
    "                           labels=['political science', 'sports', 'science']\n",
    "          - include_labels(bool): If True, will return topic labels along with topic probabilities\n",
    "          - multilabel(bool): If True, labels are considered independent and multiple labels can predicted true for document and be close to 1.\n",
    "                            If False, scores are normalized such that probabilities sum to 1.\n",
    "          - max_length(int): truncate long documents to this many tokens\n",
    "          - batch_size(int): batch_size to use. default:8\n",
    "                           Increase this value to speed up predictions - especially\n",
    "                           if len(topic_strings) is large.\n",
    "          - nli_template(str): labels are inserted into this template for use as hypotheses in natural language inference\n",
    "          - topic_strings(list): alias for labels parameter for backwards compatibility\n",
    "          \n",
    "        **Returns:**\n",
    "        \n",
    "        \n",
    "          inferred probabilities or list of inferred probabilities if doc is list\n",
    "        \"\"\"\n",
    "\n",
    "        # error checks\n",
    "        is_str_input = False\n",
    "        if not isinstance(docs, (list, np.ndarray)): \n",
    "            docs = [docs]\n",
    "            is_str_input = True\n",
    "        if not isinstance(docs[0], str): raise ValueError('docs must be string or a list of strings representing document(s)')\n",
    "        if len(labels) > 0 and len(topic_strings) > 0: raise ValueError('labels and topic_strings are mutually exclusive')\n",
    "        if not labels and not topic_strings: raise ValueError('labels must be a list of strings')\n",
    "        if topic_strings: \n",
    "            labels = topic_strings\n",
    "\n",
    "\n",
    "        # convert to sequences\n",
    "        sequence_pairs = []\n",
    "        for premise in docs:\n",
    "            sequence_pairs.extend([[premise, nli_template.format(label)] for label in labels])\n",
    "        if batch_size  > len(sequence_pairs): batch_size = len(sequence_pairs)\n",
    "        if len(sequence_pairs) >= 100 and batch_size==8:\n",
    "            warnings.warn('TIP: Try increasing batch_size to speedup ZeroShotClassifier predictions')\n",
    "        num_chunks = math.ceil(len(sequence_pairs)/batch_size)\n",
    "        sequence_chunks = list2chunks(sequence_pairs, n=num_chunks)\n",
    "\n",
    "        # inference\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            outputs = []\n",
    "            for sequences in sequence_chunks:\n",
    "                batch = self.tokenizer.batch_encode_plus(sequences, return_tensors='pt', max_length=max_length, truncation='only_first', padding=True).to(self.torch_device)\n",
    "                logits = self.model(batch['input_ids'], attention_mask=batch['attention_mask'], return_dict=False)[0]\n",
    "                outputs.extend(logits.cpu().detach().numpy())\n",
    "        outputs = np.array(outputs)\n",
    "        outputs = outputs.reshape((len(docs), len(labels), -1))\n",
    "\n",
    "        # process outputs\n",
    "        if multilabel:\n",
    "            # softmax over the entailment vs. contradiction dim for each label independently\n",
    "            entail_contr_logits = outputs[..., [0, -1]]\n",
    "            scores = np.exp(entail_contr_logits) / np.exp(entail_contr_logits).sum(-1, keepdims=True)\n",
    "            scores = scores[..., 1]\n",
    "        else:\n",
    "            # softmax the \"entailment\" logits over all candidate labels\n",
    "            entail_logits = outputs[..., -1]\n",
    "            scores = np.exp(entail_logits) / np.exp(entail_logits).sum(-1, keepdims=True)\n",
    "        scores = scores.tolist()\n",
    "        if include_labels:\n",
    "            scores = [list(zip(labels, s)) for s in scores]\n",
    "        if is_str_input: scores = scores[0]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb21694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ZeroShotClassifier.predict\" class=\"doc_header\"><code>ZeroShotClassifier.predict</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ZeroShotClassifier.predict</code>(**`docs`**, **`labels`**=*`[]`*, **`include_labels`**=*`False`*, **`multilabel`**=*`True`*, **`max_length`**=*`512`*, **`batch_size`**=*`8`*, **`nli_template`**=*`'This text is about {}.'`*, **`topic_strings`**=*`[]`*)\n",
       "\n",
       "This method performs zero-shot text classification using Natural Language Inference (NLI).\n",
       "\n",
       "\n",
       "**Parameters**:\n",
       "  - docs(list|str): text of document or list of texts\n",
       "  - labels(list): a list of strings representing topics of your choice\n",
       "                  Example:\n",
       "                   labels=['political science', 'sports', 'science']\n",
       "  - include_labels(bool): If True, will return topic labels along with topic probabilities\n",
       "  - multilabel(bool): If True, labels are considered independent and multiple labels can predicted true for document and be close to 1.\n",
       "                    If False, scores are normalized such that probabilities sum to 1.\n",
       "  - max_length(int): truncate long documents to this many tokens\n",
       "  - batch_size(int): batch_size to use. default:8\n",
       "                   Increase this value to speed up predictions - especially\n",
       "                   if len(topic_strings) is large.\n",
       "  - nli_template(str): labels are inserted into this template for use as hypotheses in natural language inference\n",
       "  - topic_strings(list): alias for labels parameter for backwards compatibility\n",
       "  \n",
       "**Returns:**\n",
       "\n",
       "\n",
       "  inferred probabilities or list of inferred probabilities if doc is list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ZeroShotClassifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05148cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl = ZeroShotClassifier()\n",
    "labels=['politics', 'elections', 'sports', 'films', 'television']\n",
    "doc = 'I am extremely dissatisfied with the President and will definitely vote in 2020.'\n",
    "preds = zsl.predict(doc, labels=labels, include_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2e002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('politics', 0.979189932346344),\n",
       " ('elections', 0.9874580502510071),\n",
       " ('sports', 0.0005765454261563718),\n",
       " ('films', 0.002292441902682185),\n",
       " ('television', 0.001054605352692306)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e98953",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(preds)\n",
    "assert d['politics'] > 0.9\n",
    "assert d['elections'] > 0.9\n",
    "assert d['sports'] < 0.1\n",
    "assert d['films'] < 0.1\n",
    "assert d['television'] < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfcbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "class TextEncoder():\n",
    "    \"\"\"\n",
    "    Tiny wrapper to sentence-transformers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='stsb-roberta-large', device=None):\n",
    "        \"\"\"\n",
    "        TextEmbedder constructor.\n",
    "\n",
    "        **Args:**\n",
    "          - model_name(str): name of fine-tuned model for embeddings\n",
    "          - device(str): device to use (e.g., 'cuda', 'cpu')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer, util\n",
    "        except ImportError:\n",
    "            raise Exception('TextEncoder requires: pip install sentence-transformers')\n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            raise Exception('PyTorch must be installed.')\n",
    "\n",
    "        self.torch_device = device\n",
    "        if self.torch_device is None: self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def encode(self, texts, batch_size=32, normalize=False, show_progress_bar=False):\n",
    "        \"\"\"Generate embedding for supplied text\"\"\"\n",
    "        if isinstance(texts, str): texts = [texts]\n",
    "        return self.model.encode(texts, batch_size=batch_size,\n",
    "                                 show_progress_bar=show_progress_bar, normalize_embeddings=normalize,\n",
    "                                 convert_to_tensor=False, device=self.torch_device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f344ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f36dcc4e2a9407b8fcb697c5dd78486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/748 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a3cfa164944d3badacdfaafe32bffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a42be657ff424ca3c7040d8258a682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471a03caea8a4f72b136f975a909b8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeafb610ad04d70b0afa33580318dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11838ee0f8194653a814babc4de04cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b8a2eb3f9846d6865cf99702a80721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0270b04f1c4e4f8b215a77496ae360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da067d773544d0896eab65b53385815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51cb39dd09646a78665f05a35e6183b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fac52a406641ae820642f39b85ea06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11451ff6bb8a49288cd3562eae916863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e721990c334e44865baedb1a7460f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfb187591044e989cf9610e49924af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "te = TextEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b53226",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = te.encode('The moon is bright.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert e.shape[0] == 1\n",
    "assert e.shape[1] == 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae14b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bec3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7635553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import math\n",
    "import numpy as np\n",
    "DEFAULT_TOKEN_PATTERN = (r\"\\b[a-zA-Z][a-zA-Z0-9]*(?:[_/&-][a-zA-Z0-9]+)+\\b|\"\n",
    "                         r\"\\b\\d*[a-zA-Z][a-zA-Z0-9][a-zA-Z0-9]+\\b\")\n",
    "\n",
    "class TopicModel():\n",
    "\n",
    "\n",
    "    def __init__(self,texts=None, n_topics=None, n_features=10000, \n",
    "                 min_df=5, max_df=0.5,  stop_words='english',\n",
    "                 model_type='lda',\n",
    "                 lda_max_iter=5, lda_mode='online',\n",
    "                 token_pattern=None, verbose=1,\n",
    "                 hyperparam_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fits a topic model to documents in <texts>.\n",
    "        Example:\n",
    "            tm = ktrain.text.get_topic_model(docs, n_topics=20, \n",
    "                                            n_features=1000, min_df=2, max_df=0.95)\n",
    "        Args:\n",
    "            texts (list of str): list of texts\n",
    "            n_topics (int): number of topics.\n",
    "                            If None, n_topics = min{400, sqrt[# documents/2]})\n",
    "            n_features (int):  maximum words to consider\n",
    "            max_df (float): words in more than max_df proportion of docs discarded\n",
    "            stop_words (str or list): either 'english' for built-in stop words or\n",
    "                                      a list of stop words to ignore\n",
    "            model_type(str): type of topic model to fit. One of {'lda', 'nmf'}.  Default:'lda'\n",
    "            lda_max_iter (int): maximum iterations for 'lda'.  5 is default if using lda_mode='online'.\n",
    "                                If lda_mode='batch', this should be increased (e.g., 1500).\n",
    "                                Ignored if model_type != 'lda'\n",
    "            lda_mode (str):  one of {'online', 'batch'}. Ignored if model_type !='lda'\n",
    "            token_pattern(str): regex pattern to use to tokenize documents. \n",
    "            verbose(bool): verbosity\n",
    "        \"\"\"\n",
    "        self.verbose=verbose\n",
    "\n",
    "        # estimate n_topics\n",
    "        if n_topics is None:\n",
    "            if texts is None:\n",
    "                raise ValueError('If n_topics is None, texts must be supplied')\n",
    "            estimated = max(1, int(math.floor(math.sqrt(len(texts) / 2))))\n",
    "            n_topics = min(400, estimated)\n",
    "            print('n_topics automatically set to %s' % (n_topics))\n",
    "\n",
    "        # train model\n",
    "        if texts is not None:\n",
    "            (model, vectorizer) = self.train(texts, model_type=model_type,\n",
    "                                             n_topics=n_topics, n_features=n_features,\n",
    "                                             min_df = min_df, max_df = max_df, \n",
    "                                             stop_words=stop_words,\n",
    "                                             lda_max_iter=lda_max_iter, lda_mode=lda_mode,\n",
    "                                             token_pattern=token_pattern,\n",
    "                                             hyperparam_kwargs=hyperparam_kwargs)\n",
    "        else:\n",
    "            vectorizer = None\n",
    "            model = None\n",
    "\n",
    "\n",
    "\n",
    "        # save model and vectorizer and hyperparameter settings\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "        self.n_topics = n_topics\n",
    "        self.n_features = n_features\n",
    "        if verbose: print('done.')\n",
    "\n",
    "        # these variables are set by self.build():\n",
    "        self.topic_dict = None\n",
    "        self.doc_topics = None\n",
    "        self.bool_array = None\n",
    "\n",
    "        self.scorer = None       # set by self.train_scorer()\n",
    "        self.recommender = None  # set by self.train_recommender()\n",
    "        return\n",
    "\n",
    "\n",
    "    def train(self,texts, model_type='lda', n_topics=None, n_features=10000,\n",
    "              min_df=5, max_df=0.5,  stop_words='english',\n",
    "              lda_max_iter=5, lda_mode='online',\n",
    "              token_pattern=None, hyperparam_kwargs=None):\n",
    "        \"\"\"\n",
    "        Fits a topic model to documents in <texts>.\n",
    "        \"\"\"\n",
    "        if hyperparam_kwargs is None:\n",
    "            hyperparam_kwargs = {}\n",
    "        alpha = hyperparam_kwargs.get('alpha', 5.0 / n_topics)\n",
    "        beta = hyperparam_kwargs.get('beta', 0.01)\n",
    "        nmf_alpha = hyperparam_kwargs.get('nmf_alpha', 0)\n",
    "        l1_ratio = hyperparam_kwargs.get('l1_ratio', 0)\n",
    "        ngram_range = hyperparam_kwargs.get('ngram_range', (1,1))\n",
    "\n",
    "         # preprocess texts\n",
    "        if self.verbose: print('preprocessing texts...')\n",
    "        if token_pattern is None: token_pattern = DEFAULT_TOKEN_PATTERN\n",
    "        #if token_pattern is None: token_pattern = r'(?u)\\b\\w\\w+\\b'\n",
    "        vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                 max_features=n_features, stop_words=stop_words,\n",
    "                                 token_pattern=token_pattern, ngram_range=ngram_range)\n",
    "        \n",
    "\n",
    "        x_train = vectorizer.fit_transform(texts)\n",
    "\n",
    "        # fit model\n",
    "\n",
    "        if self.verbose: print('fitting model...')\n",
    "        if model_type == 'lda':\n",
    "            model = LatentDirichletAllocation(n_components=n_topics, max_iter=lda_max_iter,\n",
    "                                              learning_method=lda_mode, learning_offset=50.,\n",
    "                                              doc_topic_prior=alpha,\n",
    "                                              topic_word_prior=beta,\n",
    "                                              verbose=self.verbose, random_state=0)\n",
    "        elif model_type == 'nmf':\n",
    "            model = NMF(\n",
    "                n_components=n_topics,\n",
    "                max_iter=lda_max_iter,\n",
    "                verbose=self.verbose,\n",
    "                alpha=nmf_alpha,\n",
    "                l1_ratio=l1_ratio,\n",
    "                random_state=0)\n",
    "        else:\n",
    "            raise ValueError(\"unknown model type:\", str(model_type))\n",
    "        model.fit(x_train)\n",
    "\n",
    "        # save model and vectorizer and hyperparameter settings\n",
    "        return (model, vectorizer)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def topics(self):\n",
    "        \"\"\"\n",
    "        convenience method/property\n",
    "        \"\"\"\n",
    "        return self.get_topics()\n",
    "\n",
    "\n",
    "    def get_document_topic_distribution(self):\n",
    "        \"\"\"\n",
    "        Gets the document-topic distribution.\n",
    "        \"\"\"\n",
    "        self._check_build()\n",
    "        return self.doc_topics\n",
    "\n",
    "\n",
    "    def get_sorted_docs(self, topic_id):\n",
    "        \"\"\"\n",
    "        Returns all docs sorted by relevance to <topic_id>.\n",
    "        \"\"\"\n",
    "        docs = self.get_docs()\n",
    "        d = {}\n",
    "        for doc in docs: d[doc['doc_id']] = doc\n",
    "        m = self.get_document_topic_distribution()\n",
    "        doc_ids = (-m[:,topic_id]).argsort()\n",
    "        return [d[doc_id] for doc_id in doc_ids]\n",
    "\n",
    "\n",
    "\n",
    "    def get_word_weights(self, topic_id, n_words=100):\n",
    "        \"\"\"\n",
    "        Returns a list tuples of the form: (word, weight) for given topic_id.\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        if topic_id+1 > len(self.model.components_): \n",
    "            raise ValueError('topic_id must be less than %s' % (len(self.model.components_)))\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        word_probs = self.model.components_[topic_id]\n",
    "        word_ids = [i for i in word_probs.argsort()[:-n_words - 1:-1]]\n",
    "        words = [feature_names[i] for i in word_ids]\n",
    "        probs = [word_probs[i] for i in word_ids]\n",
    "        return list( zip(words, probs) )\n",
    "\n",
    "\n",
    "    def get_topics(self, n_words=10, as_string=True):\n",
    "        \"\"\"\n",
    "        Returns a list of discovered topics\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        topic_summaries = []\n",
    "        for topic_idx, topic in enumerate(self.model.components_):\n",
    "            summary = [feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]\n",
    "            if as_string: summary = \" \".join(summary)\n",
    "            topic_summaries.append(summary)\n",
    "        return topic_summaries\n",
    "\n",
    "\n",
    "    def print_topics(self, n_words=10, show_counts=False):\n",
    "        \"\"\"\n",
    "        print topics\n",
    "        \"\"\"\n",
    "        topics = self.get_topics(n_words=n_words, as_string=True)\n",
    "        if show_counts:\n",
    "            self._check_build()\n",
    "            topic_counts = sorted([ (k, topics[k], len(v)) for k,v in self.topic_dict.items()], \n",
    "                                    key=lambda kv:kv[-1], reverse=True)\n",
    "            for (idx, topic, count) in topic_counts:\n",
    "                print(\"topic:%s | count:%s | %s\" %(idx, count, topic))\n",
    "        else:\n",
    "            for i, t in enumerate(topics):\n",
    "                print('topic %s | %s' % (i, t))\n",
    "        return\n",
    "\n",
    "\n",
    "    def build(self, texts):\n",
    "        \"\"\"\n",
    "        Builds the document-topic distribution showing the topic probability distirbution\n",
    "        \"\"\"\n",
    "        doc_topics = self.predict(texts)\n",
    "        self.doc_topics = doc_topics\n",
    "\n",
    "        self.topic_dict = self._rank_documents(texts, doc_topics=doc_topics)\n",
    "        return\n",
    "                           \n",
    "\n",
    "    \n",
    "    def get_docs(self, topic_ids=[], doc_ids=[], rank=False):\n",
    "        \"\"\"\n",
    "        Returns document entries for supplied topic_ids.           \n",
    "        \"\"\"\n",
    "        self._check_build()\n",
    "        if not topic_ids:\n",
    "            topic_ids = list(range(self.n_topics))\n",
    "        result_texts = []\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_id not in self.topic_dict: continue\n",
    "            texts = [{'text':tup[0], 'doc_id':tup[1], 'topic_proba':tup[2], 'topic_id':topic_id} for tup in self.topic_dict[topic_id] \n",
    "                                                                                                     if not doc_ids or tup[1] in doc_ids]\n",
    "            result_texts.extend(texts)\n",
    "        if not rank:\n",
    "            result_texts = sorted(result_texts, key=lambda x:x['doc_id'])\n",
    "        return result_texts\n",
    "\n",
    "\n",
    "    def get_doctopics(self,  topic_ids=[], doc_ids=[]):\n",
    "        \"\"\"\n",
    "        Returns a topic probability distribution for documents\n",
    "        \"\"\"\n",
    "        docs = self.get_docs(topic_ids=topic_ids, doc_ids=doc_ids)\n",
    "        return np.array([self.doc_topics[idx] for idx in [x['doc_id'] for x in docs]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, texts, threshold=None, harden=False):\n",
    "        \"\"\"\n",
    "        predict topics for doucments\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        transformed_texts = self.vectorizer.transform(texts)\n",
    "        X_topics = self.model.transform(transformed_texts)\n",
    "        return X_topics\n",
    "\n",
    "    def _rank_documents(self,\n",
    "                       texts,\n",
    "                       doc_topics=None):\n",
    "        \"\"\"\n",
    "        Rank documents by topic score.\n",
    "        \"\"\"\n",
    "        if doc_topics is not None:\n",
    "            X_topics = doc_topics\n",
    "        else:\n",
    "            if self.verbose: print('transforming texts to topic space...')\n",
    "            X_topics = self.predict(texts)\n",
    "        topics = np.argmax(X_topics, axis=1)\n",
    "        scores = np.amax(X_topics, axis=1)\n",
    "        doc_ids = np.array([i for i, x in enumerate(texts)])\n",
    "        result = list(zip(texts, doc_ids, topics, scores))\n",
    "        if self.verbose: print('done.')\n",
    "        result = sorted(result, key=lambda x: x[-1], reverse=True)\n",
    "        result_dict = {}\n",
    "        for r in result:\n",
    "            text = r[0]\n",
    "            doc_id = r[1]\n",
    "            topic_id = r[2]\n",
    "            score = r[3]\n",
    "            lst = result_dict.get(topic_id, [])\n",
    "            lst.append((text, doc_id, score))\n",
    "            result_dict[topic_id] = lst\n",
    "        return result_dict\n",
    "\n",
    "    \n",
    "    def _check_build(self):\n",
    "        self._check_model()\n",
    "        if self.topic_dict is None: \n",
    "            raise Exception('Must call build() method.')\n",
    "\n",
    "\n",
    "    def _check_model(self):\n",
    "        if self.model is None or self.vectorizer is None:\n",
    "            raise Exception('Must call train()')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1862b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# we only want to keep the body of the documents!\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# fetch train and test data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "\n",
    "# compile the texts\n",
    "texts = newsgroups_train.data +  newsgroups_test.data\n",
    "\n",
    "# let's also store the newsgroup category associated with each document\n",
    "# we can display this information in visualizations\n",
    "targets = [target for target in list(newsgroups_train.target) + list(newsgroups_test.target)]\n",
    "categories = [newsgroups_train.target_names[target] for target in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56986201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_topics automatically set to 97\n",
      "preprocessing texts...\n",
      "fitting model...\n",
      "iteration: 1 of max_iter: 5\n",
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "tm = TopicModel(texts, n_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 | tape adam tim case moved bag quote mass marked zionism\n",
      "topic 1 | image jpeg images format programs tiff files jfif save lossless\n",
      "topic 2 | alternative movie film static cycles films philips dynamic hou phi\n",
      "topic 3 | hell humans poster frank reality kent gerard gant eternal bell\n",
      "topic 4 | air phd chz kit cbc ups w-s rus w47 mot\n",
      "topic 5 | dog math great figure poster couldn don trying rushdie fatwa\n",
      "topic 6 | collaboration nazi fact end expression germany philly world certified moore\n",
      "topic 7 | gif points scale postscript mirror plane rendering algorithm polygon rayshade\n",
      "topic 8 | fonts font shell converted iii characters slight composite breaks compress\n",
      "topic 9 | power station supply options option led light tank plastic wall\n",
      "topic 10 | transmission rider bmw driver automatic shift gear japanese stick highway\n",
      "topic 11 | tyre ezekiel ruler hernia appeared appointed supreme man land power\n",
      "topic 12 | space nasa earth data launch surface solar moon mission planet\n",
      "topic 13 | israel jews jewish israeli arab peace war arabs palestinian kuwait\n",
      "topic 14 | olvwm xremote animals kinds roughing toolkit close corp glenn imakefile\n",
      "topic 15 | medical health disease cancer patients drug treatment drugs aids study\n",
      "topic 16 | biden chip gear like information number automatic mode insurance know\n",
      "topic 17 | graphics zip amiga shareware formats ftp gif program sgi convert\n",
      "topic 18 | brilliant mail did god coming christianity people got ideas reading\n",
      "topic 19 | black red white blue green cross wires lines helmet mask\n",
      "topic 20 | car engine cars miles clutch new ford rear slip road\n",
      "topic 21 | list mailing service model small large lists radar available major\n",
      "topic 22 | key encryption chip keys clipper phone security use government privacy\n",
      "topic 23 | talking pit nyr stl phi edm mtl wsh hfd cgy\n",
      "topic 24 | signal input switch connected circuit audio noise output control voltage\n",
      "topic 25 | stuff deleted die posting beware fantastic motives authentic reluctant hope\n",
      "topic 26 | adams douglas dc-x garrett ingres tin sdio incremental mcdonnell guide\n",
      "topic 27 | men homosexual homosexuality women gay sexual homosexuals male kinsey pop\n",
      "topic 28 | usual leo rs-232 martian reading cooperative unmanned somalia decompress visited\n",
      "topic 29 | edu university information send new computer research mail internet address\n",
      "topic 30 | reserve naval marine ret commission one-way irgun prior closure facilities\n",
      "topic 31 | state intelligence militia units army zone georgia sam croats belongs\n",
      "topic 32 | says article pain known warning doctor stone bug kidney response\n",
      "topic 33 | faq rsa ripem lights yes patent nist management wax cipher\n",
      "topic 34 | wolverine comics hulk appearance special liefeld sabretooth incredible hobgoblin x-force\n",
      "topic 35 | software ram worth cycles controller available make dram dynamic situation\n",
      "topic 36 | religion people religious catalog bobby used driven involved long like\n",
      "topic 37 | intel sites experiment ftp does know family good like mrs\n",
      "topic 38 | armenian people army russian turkish genocide armenians ottoman turks jews\n",
      "topic 39 | theft geo available face couldn cover sony people number shop\n",
      "topic 40 | christianity did exists mail matter mind tool status god reading\n",
      "topic 41 | propane probe earth orbit orbiter titan cassini space atmosphere gravity\n",
      "topic 42 | people government right think rights law make public fbi don\n",
      "topic 43 | god people does say believe bible true think evidence religion\n",
      "topic 44 | mov phone south key war supply push left just registered\n",
      "topic 45 | period goal pts play chicago pittsburgh buffalo shots new blues\n",
      "topic 46 | game team games year hockey season players player baseball league\n",
      "topic 47 | speed dod student technician just hits right note giant light\n",
      "topic 48 | sex marriage relationship family married couple depression pregnancy childhood trademark\n",
      "topic 49 | protects rejecting com4 couple decides taking connect unc nearest richer\n",
      "topic 50 | president states united american national press april washington america white\n",
      "topic 51 | card memory windows board ram bus drivers driver cpu problem\n",
      "topic 52 | window application manager display button xterm path widget event resources\n",
      "topic 53 | cable win van det bos tor cal nyi chi buf\n",
      "topic 54 | americans baltimore rochester cape springfield moncton providence utica binghamton adirondack\n",
      "topic 55 | color monitor screen mouse video colors resolution vga colour monitors\n",
      "topic 56 | option power ssf flights capability module redesign missions station options\n",
      "topic 57 | body father son vitamin diet day cells cell form literature\n",
      "topic 58 | max g9v b8f a86 bhj giz bxn biz qax b4q\n",
      "topic 59 | bit fast chip ibm faster mode chips scsi-2 speeds quadra\n",
      "topic 60 | book books law adl islam islamic iran media bullock muslims\n",
      "topic 61 | armenian russian turkish ottoman people army armenians genocide war turks\n",
      "topic 62 | oscillator partition tune nun umumiye nezareti mecmuasi muharrerat-i evrak version\n",
      "topic 63 | tongues seat est didn raise copied lazy schemes adapter leap\n",
      "topic 64 | com object jim app function motorola heterosexual objects pointers encountered\n",
      "topic 65 | effective boy projects grow jason ain dump keyboards vastly grants\n",
      "topic 66 | armenian people russian armenians turks ottoman army turkish genocide muslim\n",
      "topic 67 | mac apple pin ground wire quicktime macs pins connector simms\n",
      "topic 68 | bastard turning likes hooks notions turks cited proud pointers chuck\n",
      "topic 69 | bought dealer cost channel replaced face sony stereo warranty tube\n",
      "topic 70 | myers food reaction msg writes loop eat dee effects taste\n",
      "topic 71 | lander contradiction reconcile apparent somebody supplement essential needs produce insulin\n",
      "topic 72 | re-boost systems virginia voice unix input ken easily summary developing\n",
      "topic 73 | block tests suck shadow dte screws macedonia sunlight fin message\n",
      "topic 74 | jesus church christ god lord holy spirit mary shall heaven\n",
      "topic 75 | gun number year guns rate insurance police years new firearms\n",
      "topic 76 | rule automatically characteristic wider thumb recommendation inline mr2 halfway width\n",
      "topic 77 | drive disk hard scsi drives controller floppy ide master transfer\n",
      "topic 78 | stephanopoulos water gas oil heat energy hot temperature cold nuclear\n",
      "topic 79 | like know does use don just good thanks need want\n",
      "topic 80 | starters mlb mov higher signing left accessible argument viola teams\n",
      "topic 81 | entry rules info define entries year int printf include contest\n",
      "topic 82 | price new sale offer sell condition shipping interested asking prices\n",
      "topic 83 | issue germany title magazine german cover race generation origin nazi\n",
      "topic 84 | armenian armenians people turkish war said killed children russian turkey\n",
      "topic 85 | dos windows software comp library os/2 version microsoft applications code\n",
      "topic 86 | probe space launch titan earth cassini orbiter orbit atmosphere mission\n",
      "topic 87 | housed throws fills daylight occurring activities adjacent presenting punish occuring\n",
      "topic 88 | statement folk raids thor disarmed anatolia polygon inria arrive smehlik\n",
      "topic 89 | sound steve pro convert ati ultra fahrenheit orchid hercules blaster\n",
      "topic 90 | joke tricky wearing golden trickle seen geneva csh course caesar\n",
      "topic 91 | moral objective values morality child defined bank definition wrong different\n",
      "topic 92 | files file edu ftp available version server data use sun\n",
      "topic 93 | catalog tons seal ordering kawasaki tools fax free ultraviolet packages\n",
      "topic 94 | file program error output use section line code command problem\n",
      "topic 95 | power ssf module capability option flights redesign missions human station\n",
      "topic 96 | just don think know like time did going didn people\n"
     ]
    }
   ],
   "source": [
    "tm.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb5fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "tm.build(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2310aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d2975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.05935853, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.04939132, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.04181867, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.21681858, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.02146013, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.0458702 , 0.02146013, 0.14892628,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.00105197, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.00105197, 0.13724779, 0.00105197, 0.00105197,\n",
       "       0.00105197, 0.16612722])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.doc_topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df1975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'card memory windows board ram bus drivers driver cpu problem'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topics[ np.argmax(tm.doc_topics[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e9ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.65009096, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.06185567, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214, 0.00303214, 0.00303214, 0.00303214,\n",
       "        0.00303214, 0.00303214]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.predict(['Elon Musk leads Space Exploration Technologies (SpaceX), where he oversees '  +\n",
    "            'the development and manufacturing of advanced rockets and spacecraft for missions ' +\n",
    "            'to and beyond Earth orbit.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01571573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space nasa earth data launch surface solar moon mission planet'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topics[ np.argmax(tm.predict(['Elon Musk leads Space Exploration Technologies (SpaceX), where he oversees '  +\n",
    "            'the development and manufacturing of advanced rockets and spacecraft for missions ' +\n",
    "            'to and beyond Earth orbit.']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f7ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_causalinference.ipynb.\n",
      "Converted 01_autocoder.ipynb.\n",
      "Converted 02_analyzers.ipynb.\n",
      "Converted 03_key_driver_analysis.ipynb.\n",
      "Converted 04_preprocessing.ipynb.\n",
      "Converted 05a_meta.base.ipynb.\n",
      "Converted 05b_meta.tleaerner.ipynb.\n",
      "Converted 05c_meta.slearner.ipynb.\n",
      "Converted 05d_meta.xlearner.ipynb.\n",
      "Converted 05e_meta.rlearner.ipynb.\n",
      "Converted 05f_meta.utils.ipynb.\n",
      "Converted 05g_meta.explainer.ipynb.\n",
      "Converted 05h_meta.propensity.ipynb.\n",
      "Converted 05i_meta.sensitivity.ipynb.\n",
      "Converted 99_examples.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08054668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
